###################################################################################################
## Insieme Integration Test Script [v 2.0]
###################################################################################################
#! /bin/sh
""":"
exec python3 $0 ${1+"$@"}
"""
import optparse, collections, sys, os, re, math, time, subprocess, shlex, copy, configparser, pickle, hashlib
from multiprocessing import Pool, Process, Queue
from collections import defaultdict
from decimal import Decimal
import xml.etree.ElementTree as ET

#
# Figure out command to compute version of Code
#
INSIEME_VERSION_STR="\"\\\"@insieme_version@\\\"\""
INSIEME_VERSION="unknown"

# linker flags
LDFLAGS_GCC   = "-lm -lpthread -lrt"

CFG_FILE_NAME = 'test.cfg'
INPUTS_DATA   = 'inputs.data'
INSIEME_FLAGS = 'insieme.flags'
REF_GCC_FLAGS = 'ref-gcc.flags'
TEST_GCC_FLAGS= 'test-gcc.flags'
PROG_INPUTS   = 'prog.input'

NONE   = ''
BOLD   = '\033[1m'
UNDERLINE= '\033[4m'
GREEN  = '\033[1;32m'
YELLOW = '\033[93m'
RED    = '\033[1;31m'
BLUE   = '\033[1;94m'
ENDC   = '\033[0m'

COLUMNS=80

def col_print(msg, col=NONE) :
	if not no_colors and col != NONE:
		msg = col + msg + ENDC

	sys.stderr.write(msg)
	
"""
Generic class which represent a generic flag passed to the compiler 
"""
class Flag:
	def __init__(self, name, desc=''):
		self.name = name; self.desc = desc

	def __repr__(self):
		return self.name

"""
Specialization of a Flag which is an output flag which means any flag which 
generate an output file. 
"""
class OutFileFlag(Flag):
	def __init__(self, name, file_name, desc=''):
		super(OutFileFlag, self).__init__(name, desc)
		self.__file_name = file_name

	@property
	def file_name(self):
		return '{CASE_NAME}.' + self.__file_name

	def __repr__(self):
		return "{0} {2}/{1}".format(self.name, self.file_name, '{PATH}')

"""
A Configuration is composed by a set of flags
"""
class Conf:
	def __init__(self, flags, desc=""):
		self.desc = desc
		self.flags = flags if isinstance(flags, list) else [flags]

	def __repr__(self):
		return " ".join(map(lambda x: '{0}'.format(x), self.flags))

	def __add__(self, conf):
		tag = conf.desc
		if len(self.desc)!=0 and len(conf.desc)!=0:
			tag = self.desc + " >> " + conf.desc
		elif len(self.desc) != 0:
			tag = self.desc
		return Conf(self.flags + conf.flags, tag)

	@property
	def files(self):
		return [flag.file_name for flag in self.flags if isinstance(flag, OutFileFlag)]

	@property
	def output_file(self):
		return [flag.file_name for flag in self.flags if isinstance(flag, OutFileFlag) and flag.name == '-o']

class Pass:
	def __init__(self, frontend, tmp_file=None, backend=None, out_file=None, executor=None):
		self.frontend = copy.deepcopy(frontend)
		self.tmp_file = tmp_file
		if tmp_file and frontend:
			self.frontend.flags.append( OutFileFlag('-o', tmp_file) )
	
		if tmp_file:
			self.tmp_file = '{CASE_NAME}.'+tmp_file

		assert not (frontend and not tmp_file and backend)
		self.backend = copy.deepcopy(backend)
		self.out_file = out_file

		assert not (not backend and out_file)
		if out_file:
			self.backend.flags.append( OutFileFlag('-o', out_file) )
			self.out_file = '{CASE_NAME}.'+out_file
		
		self.executor = executor

	@property
	def files(self):
		ret = []
		if self.frontend:
			ret.extend(self.frontend.files)
		if self.backend:
			ret.extend(self.backend.files)
		return set(ret)
	
	@property
	def exec(self):
		return "{0}".format(self.executor);



class Commands:
	COMPILE = (0, 'Compile')
	SDIFF   = (1, 'SDiff')
	RUN	  	= (2, 'Run')
	EXT		= (3, 'Extract')
	HDIFF   = (4, 'HDiff')
	COPY    = (5, 'Copy')
	DEL     = (6, 'Delete')
	SUCC	= (7, 'DONE')


###################################################################################################
##
## Builds the list of test cases to be executed.
##
## This is done by searching for the file test.cfg which hould be contained in those folder grouping
## together sub benchmarks.
###################################################################################################

def read_test_cfg( path, cfg_file_name ):
	assert os.path.exists( path )

	return [path + '/' + x.strip() \
			  for x in re.split('\n',open(path+'/'+cfg_file_name).read()) \
					if len(x) > 0 and ((not x.startswith('#') or force_tests) ) and not x.strip().replace("#","",1)==""]

#printer to pretty output dicts	
def pretty(d, indent=0):
   for key, value in d.items():
      print ("\t" * indent + str(key))
      if isinstance(value, dict):
         pretty(value, indent+1)
      else:
         print ('\t' * (indent+1) + str(value))


def shortPath(path):
	basename=os.path.basename(path)
	if(len(basename)<10):
		basename=os.path.join(os.path.basename(os.path.dirname(path)),basename)
	if(len(basename)>20):
		basename="..."+basename[abs(len(basename)-17):]
	assert(len(basename)<=20)
	return basename

def expand(tests,benchSuite="IT"):
	if isinstance(tests, list):
		ret = []
		for test in tests:
			ret.extend(expand(test,benchSuite))
		return ret

	if(" #$" in tests):
		indexofSuite=tests.find(" #$")
		endofSuite=indexofSuite+len(tests[indexofSuite+3:].split(' ')[0])+3
		return expand(tests[:indexofSuite],tests[indexofSuite+3:endofSuite])
	elif(" $" in tests):
		return expand(tests[:tests.find(" $")],tests[tests.find(" $")+2:])		
	elif(force_tests and "#" in tests):
		return expand(tests[:tests.find("#")]+tests[tests.find("#")+1:len(tests.split(' ')[0])])	#remove first #
	elif("#" in tests):
		return expand(tests[:tests.find("#")-1])

	# Handle the cases where we need to recur and resolve
	if not os.path.isdir( tests ):
		return [ ]

	# If the directoy has a test.cfg file it means we have to run all the
	# contained tests
	if os.path.exists( tests+'/'+CFG_FILE_NAME ): 
		return expand( read_test_cfg( tests, CFG_FILE_NAME ),benchSuite)

	return [ tests+"$"+benchSuite ]


# Class used to store the data gathered during the execution of a benchmark 
class CmdExecReport:

	def __init__(self, type, cmd, desc, ret_code, output, memory, time, threads=-1, failState="",perfEvents=dict()):
		self.type = type
		self.cmd =  cmd
		self.desc = desc
		self.ret_code = ret_code
		self.output = output
		self.memory = memory
		self.time = time
		self.stdev = 0.0
		self.threads = threads
		self.failState=failState
		self.perfEvents=perfEvents

	def __repr__(self):
		return "{0} {1} {2} {3} {4}".format(self.type, self.cmd, self.ret_code, self.memory, self.time)



###################################################################################################
##
## TestCaseData:
##
## A class utility which is utilized to compose the name of the generated files
## from the compilation and execution of this test case
###################################################################################################
class TestData:

	def __init__(self, name, benchmarkSuite=""):
		self.__name = name
		self.reports = []
		self.benchmarkSuite=benchmarkSuite

	@property
	def path(self):		return self.__name

	@property
	def name(self):		return os.path.basename(self.__name)


global benchmarks_data
benchmarks_data = []

def print_sum(benchmarks_data) :

	entries = []
	for bench in benchmarks_data:
		entries.append( [bench.data.path] + [ (x.desc.split()[1], x.time, x.memory, x.stdev) \
				for x in bench.data.reports[:-1] if x.type[0]	== 2 ] )
	
	header = ["bench_name", "orig_time", "seq_time", "run_time", "ocl_time", 
										 "rel_seq_time",	"rel_run_time", "rel_ocl_time",  
							"orig_mem",  "seq_mem",  "run_mem", "ocl_mem",
										 "rel_seq_mem", "rel_run_mem", "rel_ocl_mem",
							"orig_stdev","seq_stdev","run_stdev","ocl_stdev", 
										 "rel_seq_stdev", "rel_run_stdev", "rel_ocl_stdev"]

	table = []
	table.append(header)
	for entry in entries:
		line = [0] * len(header)
		# add name 
		line[0] = entry[0]

		for e in entry[1:]:
			if e[0] == 'gcc':
				line[1::7] = e[1:]

			if e[0] == 'seq':
				line[2::7] = e[1:]
				line[5::7] = list(map(lambda x,y: x/y if y!=0 else 0, e[1:], line[1::7]))

			if e[0] == 'run':
				line[3::7] = e[1:]
				line[6::7] = list(map(lambda x,y: x/y if y!=0 else 0, e[1:], line[1::7]))

			if e[0] == 'ocl':
				line[4::7] = e[1:]
				line[7::7] = list(map(lambda x,y: x/y if y!=0 else 0, e[1:], line[1::7] ))

		table.append( line )
	
	print ("\n".join(map(lambda x: "{0}".format( ",".join(map(lambda y: "{0}".format(y), x) )), table)))


completed = Queue()

def median(values):
	med=0.0
	if not (len(values)==0):
		index=int((len(values)/2))
		
		if len(values)%2==0:
			if not (values[index]=="NA") and not (values[index-1]=="NA"):
				med=float(values[index]+values[index-1])/2
			else:
				return "NA"
		else:
			med=values[index]
	
	if(med=="NA"):
		return "NA"
	else:
		return round(float(med),4)

#needed to safely use format
class SafeDict(dict):
	def __missing__(self, key):
		return '{' + key + '}'

def calcRefPoint(rep):
	jobTimes=dict()
	
	#flops
	if(rep.perfEvents["FLOPS"]=="<not counted>"):
		jobTimes["f"]="NA"
	else:
		flops=int(rep.perfEvents["FLOPS"])/1000000000
		jobTimes["f"]=Decimal(flops).quantize(Decimal(10) ** -4)
		
	#memoryAmount
	if (rep.perfEvents["LL_PREFETCH_MISS"]=="<not counted>") or (rep.perfEvents["LL_CACHE_READ_MISS"]=="<not counted>") \
		or (rep.perfEvents["LL_CACHE_WRITE_MISS"]=="<not counted>"):
		jobTimes["mt"]="NA"
	else:					
		rep.perfEvents["LL_CACHE_LINE_SIZE"]=64
		memT=((int(rep.perfEvents["LL_PREFETCH_MISS"])+int(rep.perfEvents["LL_CACHE_READ_MISS"])+int(rep.perfEvents["LL_CACHE_WRITE_MISS"]))*int(rep.perfEvents["LL_CACHE_LINE_SIZE"]))/1000000000
		jobTimes["mt"]=Decimal(memT).quantize(Decimal(10) ** -4)

	#cacheHitRate
	if (rep.perfEvents["LL_CACHE_READ_MISS"]=="<not counted>" or rep.perfEvents["LL_CACHE_READ_ACCESS"]=="<not counted>" \
		or int(rep.perfEvents["LL_CACHE_READ_MISS"])==0 or int(rep.perfEvents["LL_CACHE_READ_ACCESS"])==0):
		jobTimes["cr"]="NA"
	else:
		jobTimes["cr"]=Decimal(int(rep.perfEvents["LL_CACHE_READ_MISS"])/int(rep.perfEvents["LL_CACHE_READ_ACCESS"])).quantize(Decimal(10) ** -2)	

	#refpoint
	if(jobTimes["mt"]=="NA") or (jobTimes["f"]=="NA") or (flops==0) or (memT==0):
		jobTimes["rp"]="NA"
	else:
		jobTimes["rp"]=Decimal(flops/memT).quantize(Decimal(10) ** -4)
		
	return jobTimes		

def reporter( report_queue ):
	success_count = 0
	failedTests = []
	num = 1
	latexCode=""
	txtHeaderDecl="{name}"		
	sqlHeaderDecl=""
	txtHeader="name\t\t\t"	#header for txtdocument
	suites=dict()
	
	#various static definitions
	testEnv=("\\section{HEADER}\\label{LABEL}\n"
			"\\subsection{{Runtime results}}\n"
			"{RUNTIME_TABLE}\n"
			"\\subsection{{Code statistics}}\n"
			"{STATISTICS}")
	
	tableEnv=(
		"\\begin{{table}}[H] \n"
		"\\centering\n"
		"\t\\begin{{tabular}}{COLUMN_SPEC}\n"
		"\t\t\\hline\n"
		"\t\t {COLUMN_HEADERS}\\\\\hline\n"
		"\t\t{ROWS}\n"
		"\t\\end{{tabular}}\n"
		"\\caption{CAPTION}\n"
		"\\end{{table}}\n")
	
	longTableEnv=(
		"\t\\begin{{longtable}}{COLUMN_SPEC}\n"
		"\\caption{CAPTION}\\\\\n"
		"\\hline\n"
		"\t\t {COLUMN_HEADERS}\\\\\hline\hline\n"
		"\\endfirsthead\n"
		"\\caption[]{{(continued)}}	\\\\\n"
		"\t\t\\hline\n"
		"\t\t {COLUMN_HEADERS}\\\\\hline\hline\n"
		"\endhead"
		"\t\t{ROWS}\n"
		"\t\\end{{longtable}}\n")

	usedPrefixes=("\\begin{table}[H] \n"
		"\centering\n"
		"\\begin{tabular}{|l|l|l|}\\hline \n"
		"Prefix & Description & Reference \\\\\\hline \n"
		"AP & ApexMap & \\url{http://crd.lbl.gov/groups-depts/ftg/projects/previous-projects/apex/} \\\\ \\hline \n"
		"BOT & Barcelona OpenMP Task Suite & \\url{https://pm.bsc.es/projects/bots} \\\\ \\hline \n"
		"COR & Coral Benchmarks & \\url{https://asc.llnl.gov/CORAL-benchmarks/} \\\\ \\hline \n"
		"EX & ExMatEx Benchmarks & \\url{http://codesign.lanl.gov/projects/exmatex/} \\\\ \\hline \n"
		"GAL & Excerpts of the GALPROP Code & \\\\ \\hline \n"				
		"HMPI & HOMB-MPI & \\url{http://homb.sourceforge.net/} \\\\ \\hline \n"
		"IT & Insieme Test Case &  \\\\ \\hline \n"
		"KIN & KineControl &  \\\\ \\hline \n"				
		"NAS & NAS Parallel Benchmarks & \\url{http://www.nas.nasa.gov/publications/npb.html} \\\\\\hline \n" 				
		"NPB & SNU-NPB Suite & \\url{http://aces.snu.ac.kr/Center_for_Manycore_Programming/SNU_NPB_Suite.html} \\\\ \\hline \n"
		"OEX & Common OpenMP Example Codes &  \\\\ \\hline \n"				
		"OS & OpenMP Source Code Repository & \\url{http://llc.pcg.ull.es/node/28} \\\\ \\hline \n"
		"OTBS & OpenMP Task Microbenchmark Suite & \\url{http://web.cs.uh.edu/~hpctools/home} \\\\ \\hline \n"
		"OVS & OpenMP Validation Suite & \\url{http://web.cs.uh.edu/~hpctools/home} \\\\ \\hline \n"
		"PAR & PARSEC Benchmarks & \\url{http://parsec.cs.princeton.edu/} \\\\ \\hline \n"
		"ROD & Rodinia Benchmarks & \\url{https://www.cs.virginia.edu/~skadron/wiki/rodinia/index.php/Main_Page}\\\\ \\hline\n"				
		"S2 & Splash2 & \\url{http://liuyix.org/splash2-benchmark/} \\\\ \\hline \n"
		"SEQ & Sequoia Benchmark Suite & \\url{https://asc.llnl.gov/sequoia/benchmarks/} \\\\ \\hline \n"
		"SMG & SMG2000 & \\url{https://asc.llnl.gov/computing_resources/purple/archive/benchmarks/smg/} \\\\ \\hline \n"				
		"SSC & SSCA2 (HPC Graph Analysis) & \\url{http://www.graphanalysis.org/benchmark/} \\\\ \\hline \n"
		"STR & STREAM Memory Benchmark & \\url{http://www.cs.virginia.edu/stream/} \\\\ \\hline \n"
		"VEL & Velvet & \\url{http://www.ebi.ac.uk/~zerbino/velvet/} \\\\ \\hline \n"
		"VIV & Vivid OpenCL Benchmarks & \\url{https://github.com/mertdikmen/ViVid} \\\\\\hline \n"
		"\\end{tabular} \n"
		"\\caption{Used Prefixes} \n"
		"\\end{table} \n"
	)
	
	notationTable=("\\begin{table}[H] \n"
		"\centering\n"
		"\\begin{tabular}{|l|l|}\\hline \n"
		"Shortcut & Explanation\\\\\\hline \n"
		"LOC & Lines of sourcecode \\\\\\hline \n"
		"size & Size of sourcecode \\\\\\hline \n"
		"parType & Type of parallelization approach: \\\\ & either number of openmp pragmas, OPENCL, MPI or sequential\\\\\\hline \n"		
		"GFLOPS & Gigaflops per second \\\\\\hline \n"
		"memTrans & Amount of transferred gigagbytes to main memory (after cache)\\\\\\hline \n"
		"boundness & Factor showing if the test is memory or compute bound: \\\\& negative value depicts memory boundness, positive value compute boundness. \\\\\\hline \n"
		"CMR & Cache miss rate of the last level cache \\\\\\hline \n"
		"failsAt & If the test fails here the station of the fail is written\\\\\\hline \n"
		"seqRun & Sequential runtime using insieme seq backend\\\\\\hline \n"
		"effGcc & Efficiencies of the application using GCC. \\\\ & The efficiencies are calculated using the runs specified as command line arg. \\\\\\hline \n"
		"effIns & Efficiencies of the application using Insieme\\\\\\hline \n"
		"memGcc & Main memory consumption of the application using GCC.\\\\\\hline \n"
		"memIns & Main memory consumption of the application using Insieme.\\\\\\hline \n"
		"\\end{tabular} \n"
		"\\caption{Notations} \n"
		"\\end{table} \n"						
	)
	
	sqlCreate=("\n"
		"PRAGMA foreign_keys = ON;\n"		#enable foreign keys (for sqlite)
		"CREATE TABLE IF NOT EXISTS Test (\n"
		"	id varchar(20) PRIMARY KEY,\n"
		"	name varchar(50),\n"
		"	path varchar(200),\n"
		"	repository varchar(3),\n"
		"	codeType varchar(3),\n"
		"	loc int,\n"
		"	size int,\n"
		"	ompPragmas int,\n"
		"	parType varchar(10),\n"
		"	failState varchar(30)\n"
		");\n"
		"CREATE TABLE IF NOT EXISTS RunConfiguration(\n"
		"	id varchar(20) PRIMARY KEY,\n"
		"	testID varchar(20) REFERENCES Test(id) ON DELETE CASCADE,\n"
		"	type varchar(10),\n"
		"	numThreads int,\n"
		"	insiemeVersion varchar(20) REFERENCES InsiemeVersion(name) ON DELETE CASCADE,\n"
		"	backendCompiler varchar(20) REFERENCES BackendCompiler(id) ON DELETE CASCADE,\n"
		"	host varchar(20) REFERENCES Host(name) ON DELETE CASCADE,\n"
		"	UNIQUE(testID,type,numThreads)"
		");\n"
		"CREATE TABLE IF NOT EXISTS Metric(\n"
		"	name varchar(20) PRIMARY KEY\n"
		");\n"
		"CREATE TABLE IF NOT EXISTS Result(\n"
		"	id varchar(20) PRIMARY KEY,"
		"	metricID varchar(20) REFERENCES Metric(name) ON DELETE SET NULL,\n"
		"	runID varchar(20) REFERENCES RunConfiguration(id) ON DELETE CASCADE,\n"
		"	value FLOAT\n,"
		"	executionDate timestamp DEFAULT CURRENT_TIMESTAMP\n"
		");\n"
		"CREATE TABLE IF NOT EXISTS InsiemeVersion(\n"
		"	name varchar(20) PRIMARY KEY,\n"
		"	firstSeen timestamp DEFAULT CURRENT_TIMESTAMP\n"
		");\n"
		"CREATE TABLE IF NOT EXISTS BackendCompiler(\n"
		"	id varchar(20) PRIMARY KEY,\n"
		"	name varchar(20),\n"
		"	version varchar(20)"
		");\n"
		"CREATE TABLE IF NOT EXISTS Host(\n"
		"	name varchar(20) PRIMARY KEY,\n"
		"	kernelVersion varchar(20),\n"
		"	rooflinePoint float\n"
		");\n"
	)
	
	sqlInsertTest=("\n"
		"INSERT OR IGNORE INTO Test (id,name,path,repository,codeType,loc,size,ompPragmas,parType,failState) "
		"VALUES(\"{ID}\",\"{NAME}\",\"{PATH}\",\"{REPOSITORY}\",\"{CODETYPE}\",{LOC},{SIZE},{OMPPRAGMAS},\"{PARTYPE}\",\"{FAILSTATE}\");\n"
	)
	
	sqlInsertRunConfiguration=("\n"
		"INSERT OR IGNORE INTO RunConfiguration(id,testID,type,numThreads,insiemeVersion,backendCompiler,host) "
		"VALUES(\"{ID}\",\"{TESTID}\",\"{TYPE}\",{NUMTHREADS},\"{INSIEMEVERSION}\",\"{BACKENDCOMPILER}\",\"{HOST}\");\n"
	)

	sqlInsertMetric=("INSERT OR IGNORE INTO Metric(name) VALUES(\"{NAME}\");\n")
	sqlInsertResult=("INSERT INTO Result(id,metricID,runID,value,executionDate) "
		"VALUES(\"{ID}\",\"{METRICID}\",\"{RUNID}\",{VALUE},\"{EXECUTIONDATE}\");\n")
	sqlInsertInsiemeVersion=("INSERT OR IGNORE INTO InsiemeVersion(name) "
		"VALUES(\"{NAME}\");\n")
	sqlInsertBackendCompiler=("INSERT OR IGNORE INTO BackendCompiler(id,name,version) "
		"VALUES(\"{ID}\",\"{NAME}\",\"{VERSION}\");\n")
	sqlInsertHost=("INSERT OR IGNORE INTO Host(name,kernelVersion,rooflinePoint) "
		"VALUES(\"{NAME}\",\"{KERNELVERSION}\",{ROOFLINEPOINT});\n")	
	
	sqlDeleteTest=("DELETE FROM Test where id=\"{ID}\";\n")
	
	headerWritten=False
	if(enable_stat and 'txt' in stat_output and not mock_run):
		fileTxt = open("statistics.txt", "a")
		fileTxt.write("\n############################### RUN "+time.strftime("%d/%m/%Y %H:%M")+" ###############################\n\n")
			
	if(enable_stat and 'csv' in stat_output and not mock_run):
		if(os.path.exists("statistics.csv")):
			os.remove("statistics.csv")
		fileCsv=open("statistics.csv","w")
			
	if(enable_stat and 'sql' in stat_output and not mock_run):
		if(os.path.exists("statistics.sql")):
			os.remove("statistics.sql")
		fileSql=open("statistics.sql","w")
		
		#INSERT INSIEME VERSION
		#insVersion=insVersion
		insertVersion=sqlInsertInsiemeVersion.format(**({
			'NAME':INSIEME_VERSION
		}))
		
		#INSERT HOST
		(retcode, output) = exec_shell_cmd("hostname")
		host=output[:-1]
		(retcode, output) = exec_shell_cmd("uname -r")
		kernelV=output[:-1]
		insertHost=sqlInsertHost.format(**({
			'NAME':host,
			'KERNELVERSION':kernelV,
			'ROOFLINEPOINT':perf_metrics['REFPOINT']
		}))
		
		fileSql.write(sqlCreate+insertVersion+insertHost)
		fileSql.flush()	
		
		#stores all metrics present in the db
		knownMetrics=list()
			
	if(enable_stat and 'bin' in stat_output and not mock_run):
		if(os.path.exists("statistics.bin")):
			os.rename("statistics.bin","statistics_bak.bin")
		fileBin=open("statistics.bin","wb")
			
	while True:
		job = completed.get()
		
		jobName=""
		jobTimes=collections.OrderedDict()
		
		# The work is completed, exit the loop and print the summary 
		if job is None: break

		totalTimes=collections.OrderedDict()

		cached=False		#set to true if totalTimes and statistics already built
		cachedMark=""
		if not(job.statistics==None):
			statistics=job.statistics
			cached=True
			cachedMark=" -- CACHED"
		else:
			statistics=[]
		
		sep = "#"+"-"*(COLUMNS-2)+"#\n"
		col_print("\n"+sep, BOLD)
		col_print("# {0:>3}/{1:<3}: {2:<{3}} #\n".format(num, test_cases_number, job.data.path+cachedMark,
			COLUMNS-13), BOLD)
		col_print(sep, BOLD)
						
		success = job.data.reports[-1].desc == 'DONE' and job.data.reports[-1].ret_code == 0

		failState=""
		#determine why test fails
		if not success:
			failState= job.data.reports[-1].failState
			#print("fs:"+failState)
		
		# Store the result data for future evaluations 
		benchmarks_data.append( job )
		
		# Check the output for all the selected backends
		for rep in job.data.reports[:-1]:
			# Cmds with an id greater than 3 are not important to be shown 
			if rep.type[0] > 4:
				continue
		
			print(rep)
			omp=True
			if rep.threads==-1:
				omp=False
			
			#cached file has more thread runs than needed, skipping
			if omp and cached and int(str(rep.threads).split(";")[-1])>max(threadRuns):
				continue

			#log times of different omp threads
			if omp and rep.type==Commands.RUN:
				if not (jobName==rep.desc):
					if(len(jobTimes)>0):
						totalTimes[jobName]=jobTimes.copy()

					jobName=rep.desc
					jobTimes.clear()
				
				if("FLOPS" in rep.perfEvents.keys()):
					 help=calcRefPoint(rep)
					 jobTimes["f"+str(rep.threads).split(";")[-1]]=help["f"]
					 jobTimes["mt"+str(rep.threads).split(";")[-1]]=help["mt"]
					 jobTimes["rp"+str(rep.threads).split(";")[-1]]=help["rp"]
					 jobTimes["cr"+str(rep.threads).split(";")[-1]]=help["cr"]
					 
				jobTimes["r"+str(rep.threads).split(";")[-1]]=Decimal(rep.time).quantize(Decimal(10) ** -2)
				jobTimes["m"+str(rep.threads).split(";")[-1]]=Decimal(rep.memory/1024,2).quantize(Decimal(10) ** -2)
				
				for key,val in rep.perfEvents.items():
					jobTimes[key+str(rep.threads)]=val
				#calculate speedup/efficiency
				if not(str(rep.threads).startswith(("dyn","stat","guid"))):	#no efficiency/speedup for schedule runs
					if(rep.threads>1):
						if(rep.time==0):
							jobTimes["s"+str(rep.threads)]="NA"
							jobTimes["e"+str(rep.threads)]="NA"						
						else:
							jobTimes["s"+str(rep.threads)]=Decimal(jobTimes["r1"]/Decimal(rep.time)).quantize(Decimal(10) ** -2)
							jobTimes["e"+str(rep.threads)]=Decimal(jobTimes["s"+str(rep.threads)]/rep.threads).quantize(Decimal(10) ** -2)
			elif rep.type==Commands.RUN:
				totalTimes[jobName]=jobTimes.copy()
				jobTimes.clear()
				jobName=rep.desc
				jobTimes["r1"]=Decimal(rep.time).quantize(Decimal(10) ** -2)
				jobTimes["m1"]=Decimal(round(rep.memory/1024,2)).quantize(Decimal(10) ** -2)
				
				#perf performance statistics and comp/mem bound refPoint
				if("FLOPS" in rep.perfEvents.keys()):
					 help=calcRefPoint(rep)			
					 jobTimes["f1"]=help["f"]
					 jobTimes["mt1"]=help["mt"]
					 jobTimes["rp1"]=help["rp"]					
					 jobTimes["cr1"]=help["cr"]					

				for key,val in rep.perfEvents.items():
					jobTimes[key+"1"]=val

			rep.desc_omp=rep.desc
			if omp:
				rep.desc_omp=rep.desc+" "+str(rep.threads).split(";")[-1]+" threads"	
			
			#output
			if rep.ret_code == 0:
				col_print("# {0:<{1}}".format(rep.desc_omp, COLUMNS-18), NONE),
				col_print("[{0:>6.2f} secs {1}, {2:>6.2f} MB]\n".
					format(rep.time, "DEV({0:.4f})".format(rep.stdev) if rep.stdev != 0 else "",round(rep.memory/1024,2)),
					BLUE if rep.type == Commands.RUN else BOLD)
			else:
				col_print("# {0:<{1}}[{2:>8.4f} secs {3}, {4:>6.2f} MB]\n".
					format( rep.desc_omp, COLUMNS-18, rep.time, "DEV({0:.4f})".format(rep.stdev) if rep.stdev != 0 else "",round(rep.memory/1024,2)), RED)
				col_print(sep, RED)
				
				if rep.output is "" or rep.output is None:
					sys.stderr.write('Program exits with error code {0}\n'.format(rep.ret_code))
				else:
					# Prints the failed command 
					col_print("Command: ")
					col_print(rep.cmd+"\n\n", GREEN)
					sys.stderr.write( '\n'.join(str(rep.output).split('\n')[0:15 if not verbose else -1]) )

				col_print("\n"+sep, RED)
				
		if not success:
			col_print(sep, RED)
			col_print("#{0:^{1}}#\n".format("FAILED -- " + job.data.path, COLUMNS-2), RED)
			col_print(sep, RED)
			failedTests.append( job.data.path )
		else:
			col_print(sep, GREEN)
			col_print("#{0:^{1}}#\n".format("SUCCESS -- " + job.data.path, COLUMNS-2), GREEN)
			col_print(sep, GREEN)
			success_count += 1
			
		sys.stderr.flush()
		num += 1			
			
		
		if(enable_stat and not mock_run):
			if not len(jobTimes)==0:
				totalTimes[jobName]=jobTimes.copy()
			
			suite=job.data.benchmarkSuite
			
			#initialize overViewStatisticTable
			if not suite in suites.keys():
				suites[suite]=dict()
			suites[suite][job.data.path]=dict()
			suites[suite][job.data.path]["GCCMinEff"]=""
			suites[suite][job.data.path]["GCCMaxEff"]=""
			suites[suite][job.data.path]["GCCMedEff"]=""
			suites[suite][job.data.path]["InsMinEff"]=""
			suites[suite][job.data.path]["InsMaxEff"]=""
			suites[suite][job.data.path]["InsMedEff"]=""
			suites[suite][job.data.path]["CompEff"]=""			
			suites[suite][job.data.path]["GCCMinMem"]=""			
			suites[suite][job.data.path]["GCCMedMem"]=""			
			suites[suite][job.data.path]["GCCMaxMem"]=""			
			suites[suite][job.data.path]["InsMinMem"]=""			
			suites[suite][job.data.path]["InsMedMem"]=""			
			suites[suite][job.data.path]["InsMaxMem"]=""			
			suites[suite][job.data.path]["seqRun"]=""
			suites[suite][job.data.path]["Dyn/Stat"]=""
			suites[suite][job.data.path]["Guid/Stat"]=""
			suites[suite][job.data.path]["Stat/Stat"]=""
			suites[suite][job.data.path]["CodeStyle"]=""
			suites[suite][job.data.path]["flops"]=""
			suites[suite][job.data.path]["memTrans"]=""
			suites[suite][job.data.path]["boundness"]=""
			suites[suite][job.data.path]["cacheRate"]=""
			
			header=job.data.path
						
			#transform keys to better readable stuff
			if("Run c_run" in totalTimes):
				totalTimes["Insieme"]=totalTimes.pop("Run c_run")
			if("Run gcc" in totalTimes):
				totalTimes["GCC"]=totalTimes.pop("Run gcc")
			if("Run g++" in totalTimes):
				totalTimes["G++"]=totalTimes.pop("Run g++")
			if("Run cxx_seq" in totalTimes):
				totalTimes["Sequential"]=totalTimes.pop("Run cxx_seq")
			if("Run cxx_run" in totalTimes):
				totalTimes["Insieme"]=totalTimes.pop("Run cxx_run")
			if("Run cxx11_seq" in totalTimes):
				totalTimes["Sequential"]=totalTimes.pop("Run cxx11_seq")
			if("Run cxx11_run" in totalTimes):
				totalTimes["Insieme"]=totalTimes.pop("Run cxx11_run")
			if("Run c_seq" in totalTimes):
				totalTimes["Sequential"]=totalTimes.pop("Run c_seq")			
			if("Run mpi" in totalTimes):
				totalTimes["MPI"]=totalTimes.pop("Run mpi")
			if("Run c_run_dyn" in totalTimes):
				dyn=totalTimes.pop("Run c_run_dyn")
				if(enable_sched):
					totalTimes["Dyn"]=dyn
			if("Run c_run_stat" in totalTimes):
				stat=totalTimes.pop("Run c_run_stat")
				if(enable_sched):
					totalTimes["Stat"]=stat
			if("Run c_run_guid" in totalTimes):
				guid=totalTimes.pop("Run c_run_guid")
				if(enable_sched):
					totalTimes["Guid"]=guid
			if("Run insiemecc" in totalTimes):
				totalTimes["ICC"]=totalTimes.pop("Run insiemecc")					

			#set code style, assume that mpi code is c++
			if("G++" in totalTimes or "MPI" in totalTimes or failState=="G++"):
				suites[suite][job.data.path]["CodeStyle"]="C++"
			if("GCC" in totalTimes or failState=="GCC"):
				suites[suite][job.data.path]["CodeStyle"]="C"			
			
			suites[suite][job.data.path]["failState"]=failState
			
			rows=""		#row for latex table

			numThreadRuns=len(threadRuns)
			colheaders=(""
				"~ & \multicolumn{THREADS}{{|c||}}{{Runtime}} & \multicolumn{THREADS-1}{{|c||}}{{Efficiency}}"
				" & \multicolumn{THREADS-1}{{|c||}}{{Speedup}} & \multicolumn{THREADS}{{|c||}}{{Memory}} & \\multicolumn{{1}}{{c|}}{{medEff}} & \\multicolumn{{1}}{{c|}}{{medMem}} \\\\\nThreads & "
			).format(**({'THREADS':"{"+str(numThreadRuns)+"}",'THREADS-1':"{"+str(numThreadRuns-1)+"}"}))

			colspec="{|r"
			rowDecl=" & "
			rowDeclPrefix=["r","e","s","m"]
			
			for k in range(0,4):
				colspec=colspec+"|"
				if(k == 0 or k==3):
					rowDecl=rowDecl+"{"+rowDeclPrefix[k]+"1} & "
					colspec=colspec+"|r"
					colheaders=colheaders+"\\multicolumn{1}{|c|}{1} & "
				for i in range(1,numThreadRuns-1):
					rowDecl=rowDecl+"{"+rowDeclPrefix[k]+str(pow(2,i%(numThreadRuns)))+"} & "
					colspec=colspec+"|r"
					colheaders=colheaders+"\\multicolumn{1}{|c|}{"+str(pow(2,i%(numThreadRuns)))+"} & "
				colspec=colspec+"|r"
				rowDecl=rowDecl+"{"+rowDeclPrefix[k]+str(max(threadRuns))+"} &"
				colheaders=colheaders+"\\multicolumn{1}{|c||}{"+str(max(threadRuns))+"} & "

			#create headers for txt/csv
			if not headerWritten:
				for k in threadRuns:
					txtHeaderDecl=txtHeaderDecl+"{INSr"+str(k)+"}\t"
					txtHeaderDecl=txtHeaderDecl+"{INSm"+str(k)+"}\t"
					txtHeaderDecl=txtHeaderDecl+"{ICCr"+str(k)+"}\t"
					txtHeaderDecl=txtHeaderDecl+"{ICCm"+str(k)+"}\t"
					txtHeaderDecl=txtHeaderDecl+"{GCCr"+str(k)+"}\t"
					txtHeaderDecl=txtHeaderDecl+"{GCCm"+str(k)+"}\t"
					
					txtHeader=txtHeader+"RUNr"+str(k)+"\t"
					txtHeader=txtHeader+"RUNm"+str(k)+"\t"
					txtHeader=txtHeader+"ICCr"+str(k)+"\t"
					txtHeader=txtHeader+"ICCm"+str(k)+"\t"
					txtHeader=txtHeader+"GCCr"+str(k)+"\t"
					txtHeader=txtHeader+"GCCm"+str(k)+"\t"
								
				maxThread=str(max(threadRuns))
				if("insieme" in totalTimes):
					if(("r"+maxThread) in totalTimes["Insieme"]):
						if not (totalTimes["Insieme"]["r"+maxThread]==0) and enable_sched:
							if("Dyn" in totalTimes):
								txtHeader=txtHeader+"DYNr"+maxThread+"\t"
								txtHeader=txtHeader+"DYNm"+maxThread+"\t"
								txtHeaderDecl=txtHeaderDecl+"{DYNr"+maxThread+"}\t"	
								txtHeaderDecl=txtHeaderDecl+"{DYNm"+maxThread+"}\t"	
							if("Guid" in totalTimes):
								txtHeader=txtHeader+"GUIr"+maxThread+"\t"
								txtHeader=txtHeader+"GUIm"+maxThread+"\t"
								txtHeaderDecl=txtHeaderDecl+"{GUIr"+maxThread+"}\t"									
								txtHeaderDecl=txtHeaderDecl+"{GUIm"+maxThread+"}\t"			
							if("Stat" in totalTimes):
								txtHeader=txtHeader+"STAr"+maxThread+"\t"
								txtHeader=txtHeader+"STAm"+maxThread+"\t"
								txtHeaderDecl=txtHeaderDecl+"{STAr"+maxThread+"}\t"									
								txtHeaderDecl=txtHeaderDecl+"{STAm"+maxThread+"}\t"			
				
				txtHeader=txtHeader+"SEQr1\t"
				txtHeader=txtHeader+"SEQm1\t"
				
				if(enable_perf):
					txtHeader=txtHeader+"GFLOPS\t"
					txtHeader=txtHeader+"mem(GB)\t"
					txtHeader=txtHeader+"refP\t"
					txtHeader=txtHeader+"caRate\t"
				
				txtHeaderDecl=txtHeaderDecl+"{SEQr1}\t"
				txtHeaderDecl=txtHeaderDecl+"{SEQm1}\t"
				
				if(enable_perf):
					txtHeaderDecl=txtHeaderDecl+"{GCCf1}\t"
					txtHeaderDecl=txtHeaderDecl+"{GCCmt1}\t"
					txtHeaderDecl=txtHeaderDecl+"{GCCrp1}\t"
					txtHeaderDecl=txtHeaderDecl+"{GCCcr1}\t"
				
			colspec=colspec+"||r|r|}"								
			colheaders=colheaders+" & "
			txtRow=txtHeaderDecl
		#	sqlRow=sqlHeaderDecl
			rows="\\hline "
			
			for k,v in totalTimes.items():
				effs=[]
				mems=[]
				rows=rows+k+rowDecl.format_map(defaultdict(str,v.items()))
				
				for key,value in v.items():
					if key.startswith("e"):		#collect efficiencies for median calculation
						if not value=="NA":
							effs.append(value)	
					elif key.startswith("m") and not key.startswith("mt"):
						if not value=="NA":
							mems.append(value)	

				totalTimes[k]["maxEff"]=""
				totalTimes[k]["minEff"]=""
				medEff=""
				if(len(effs)>0):
					medEff=median(effs)
					totalTimes[k]["maxEff"]=max(effs)
					totalTimes[k]["minEff"]=min(effs)
					if not (medEff=="NA"):
						medEff=Decimal(median(effs)).quantize(Decimal(10) ** -3)
				rows=rows+str(medEff)
				totalTimes[k]["medEff"]=medEff
				
				totalTimes[k]["maxMem"]=""
				totalTimes[k]["minMem"]=""				
				medMem=""
				if(len(mems)>0):
					totalTimes[k]["maxMem"]=max(mems)
					totalTimes[k]["minMem"]=min(mems)
					medMem=median(mems)
					if not (medMem=="NA"):
						medMem=Decimal(median(mems)).quantize(Decimal(10) ** -3)
				rows=rows+" & "+str(medMem)
				totalTimes[k]["medMem"]=medMem
				rows=rows+"\\\\\\hline\n\t\t"
				txtRow=txtRow.format_map(SafeDict(dict((k[:3].upper()+str(key),str(val)[:7]) for key,val in v.items())))

			suites[suite][job.data.path]["Type"]="ompOrSeq"		#i don't know yet if seq or omp	

			if(hasattr(job,'cur_passes')):
				for tmp in job.cur_passes:
					if(tmp[0]=='ocl'):
						suites[suite][job.data.path]["Type"]="OCL"
						break
					if(tmp[0]=='mpi'):
						suites[suite][job.data.path]["Type"]="MPI"
						break

			gccVariant=""
			if("GCC" in totalTimes):	
				gccVariant="GCC"
			elif("G++" in  totalTimes):
				gccVariant="G++"
			elif("CPP" in totalTimes):
				gccVariant="CPP"
			elif("CPP11" in totalTimes):
				gccVariant="CPP11"
			elif("MPI" in totalTimes):
				gccVariant="MPI"
			
			if not gccVariant=="":
				if "f1" in totalTimes[gccVariant].keys():
					suites[suite][job.data.path]["flops"]=totalTimes[gccVariant]["f1"]							
				if "mt1" in totalTimes[gccVariant].keys():
					suites[suite][job.data.path]["memTrans"]=str(totalTimes[gccVariant]["mt1"])+" GB"
				if "rp1" in totalTimes[gccVariant].keys():
					if(totalTimes[gccVariant]["rp1"]!="NA"):
						boundness=Decimal(float(totalTimes[gccVariant]["rp1"])-float(perf_metrics["REFPOINT"])).quantize(Decimal(10)**-3)
						if (boundness<0):
							suites[suite][job.data.path]["boundness"]=str(boundness)+" (MEM)"
						elif (boundness>0):
							suites[suite][job.data.path]["boundness"]=str(boundness)+" (COMP)"						
						else:
							suites[suite][job.data.path]["boundness"]=str(boundness)
										
				if "cr1" in totalTimes[gccVariant].keys():
					suites[suite][job.data.path]["cacheRate"]=totalTimes[gccVariant]["cr1"]					
				if "medEff" in totalTimes[gccVariant].keys():
					suites[suite][job.data.path]["GCCMedEff"]=totalTimes[gccVariant]["medEff"]
				if "medMem" in totalTimes[gccVariant].keys():
					suites[suite][job.data.path]["GCCMedMem"]=totalTimes[gccVariant]["medMem"]
				if "maxEff" in totalTimes[gccVariant].keys():
					suites[suite][job.data.path]["GCCMaxEff"]=totalTimes[gccVariant]["maxEff"]
				if "maxMem" in totalTimes[gccVariant].keys():
					suites[suite][job.data.path]["GCCMaxMem"]=totalTimes[gccVariant]["maxMem"]
				if "minEff" in totalTimes[gccVariant].keys():
					suites[suite][job.data.path]["GCCMinEff"]=totalTimes[gccVariant]["minEff"]
				if "minMem" in totalTimes[gccVariant].keys():
					suites[suite][job.data.path]["GCCMinMem"]=totalTimes[gccVariant]["minMem"]				
				
				
			if("Insieme" in totalTimes) and omp:
				maxThread="r"+str(max(threadRuns))
				if "medEff" in totalTimes["Insieme"].keys():
					suites[suite][job.data.path]["InsMedEff"]=totalTimes["Insieme"]["medEff"]
				if "medMem" in totalTimes["Insieme"].keys():
					suites[suite][job.data.path]["InsMedMem"]=totalTimes["Insieme"]["medMem"]
				if "minEff" in totalTimes["Insieme"].keys():
					suites[suite][job.data.path]["InsMinEff"]=totalTimes["Insieme"]["minEff"]
				if "minMem" in totalTimes["Insieme"].keys():
					suites[suite][job.data.path]["InsMinMem"]=totalTimes["Insieme"]["minMem"]					
				if "maxEff" in totalTimes["Insieme"].keys():
					suites[suite][job.data.path]["InsMaxEff"]=totalTimes["Insieme"]["maxEff"]
				if "maxMem" in totalTimes["Insieme"].keys():
					suites[suite][job.data.path]["InsMaxMem"]=totalTimes["Insieme"]["maxMem"]					
					
				if(maxThread in totalTimes["Insieme"]):
					if not (totalTimes["Insieme"][maxThread]==0) and enable_sched:
						if("Dyn" in totalTimes):
							suites[suite][job.data.path]["Dyn/Stat"]=Decimal(totalTimes["Dyn"][maxThread]/totalTimes["Insieme"][maxThread]).quantize(Decimal(10)**-3)
						if("Guid" in totalTimes):
							suites[suite][job.data.path]["Guid/Stat"]=Decimal(totalTimes["Guid"][maxThread]/totalTimes["Insieme"][maxThread]).quantize(Decimal(10)**-3)
						if("Stat" in totalTimes):
							suites[suite][job.data.path]["Stat/Stat"]=Decimal(totalTimes["Stat"][maxThread]/totalTimes["Insieme"][maxThread]).quantize(Decimal(10)**-3)	
								
			if("Insieme" in totalTimes.keys()):
				suites[suite][job.data.path]["seqRun"]=totalTimes["Insieme"]["r1"]
			
			caption="{ Runtime results for "+os.path.abspath(header).replace("_","\_")+"}"

			if(not rows=="\\hline "):
				tableRuntime=tableEnv.format(**({'COLUMN_SPEC':colspec,'COLUMN_HEADERS':colheaders,'ROWS':rows,'CAPTION':caption}))
			else:
				tableRuntime="No times available!"
			
			if not cached:
				statLOC=LOCStatistics("",job,"LOC")
				statistics.append(statLOC)
				statistics.append(Statistic("du -chL {INPUT} | grep \"total\" | awk '{{print $1;}}'",job,"Size of Source","size"))
			
				#omp statistics
				statNumPragmas=Statistic("egrep '#pragma omp' {INPUT} | wc -l",job,"Num omp pragmas","numOmp" )
				statistics.append(statNumPragmas)
				statistics.append(Statistic("egrep '#pragma[[:blank:]]*omp[[:blank:]]*for' {INPUT} | wc -l",job,"\\hspace{0.5cm}Num omp for ","ompFor" ,True))
				statistics.append(Statistic("egrep '#pragma[[:blank:]]*omp[[:blank:]]*parallel' {INPUT} | egrep -v '#pragma[[:blank:]]*omp[[:blank:]]*parallel[[:blank:]]*for'| egrep -v '#pragma[[:blank:]]*omp[[:blank:]]*parallel[[:blank:]]*sections' | wc -l",job,"\\hspace{0.5cm}Num omp parallel","ompPar" ,True))
				statistics.append(Statistic("egrep '#pragma[[:blank:]]*omp[[:blank:]]*task' {INPUT} | egrep -v '#pragma[[:blank:]]*omp[[:blank:]]*taskwait' | wc -l",job,"\\hspace{0.5cm}Num omp task","ompTask" ,True))
				statistics.append(Statistic("egrep '#pragma[[:blank:]]*omp[[:blank:]]*parallel[[:blank:]]*for' {INPUT} | wc -l",job,"\\hspace{0.5cm}Num omp parallel for","ompFor" ,True))
				statistics.append(Statistic("egrep '#pragma[[:blank:]]*omp[[:blank:]]*parallel[[:blank:]]*sections' {INPUT} | wc -l",job,"\\hspace{0.5cm}Num omp parallel sections","ompSect",True ))
				statistics.append(Statistic("egrep '#pragma[[:blank:]]*omp[[:blank:]]*critical' {INPUT} | wc -l",job,"\\hspace{0.5cm}Num omp critical","ompCrit" ,True))
				statistics.append(Statistic("egrep '#pragma[[:blank:]]*omp[[:blank:]]*atomic' {INPUT} | wc -l",job,"\\hspace{0.5cm}Num omp atomic","ompAtom" ,True))
				statistics.append(Statistic("egrep '#pragma[[:blank:]]*omp[[:blank:]]*single' {INPUT} | wc -l",job,"\\hspace{0.5cm}Num omp single","ompSing" ,True))
				statistics.append(Statistic("egrep '#pragma[[:blank:]]*omp[[:blank:]]*master' {INPUT} | wc -l",job,"\\hspace{0.5cm}Num omp master","ompMas" ,True))
				statistics.append(Statistic("egrep '#pragma[[:blank:]]*omp[[:blank:]]*barrier' {INPUT} | wc -l",job,"\\hspace{0.5cm}Num omp barrier","ompBarr" ,True))
				statistics.append(Statistic("egrep '#pragma[[:blank:]]*omp[[:blank:]]*flush' {INPUT} | wc -l",job,"\\hspace{0.5cm}Num omp flush","ompFlush" ,True))
				statistics.append(Statistic("egrep '#pragma[[:blank:]]*omp[[:blank:]]*taskwait' {INPUT} | wc -l",job,"\\hspace{0.5cm}Num omp taskwait","ompTaskwait" ,True))			
				statistics.append(Statistic("egrep '#pragma[[:blank:]]*omp[[:blank:]]*threadprivate' {INPUT} | wc -l",job,"\\hspace{0.5cm}Num omp threadprivate","ompThreadpriv" ,True))
				statistics.append(Statistic("(egrep '#pragma[[:blank:]]*omp[[:blank:]]*for' {INPUT} && egrep '#pragma[[:blank:]]*omp[[:blank:]]*parallel[[:blank:]]*for' {INPUT})| egrep 'schedule[[:blank:]]*\([[:blank:]]*static' | wc -l" ,job,"Num static schedule","scheduleStat" ,True))
				statistics.append(Statistic("(egrep '#pragma[[:blank:]]*omp[[:blank:]]*for' {INPUT} && egrep '#pragma[[:blank:]]*omp[[:blank:]]*parallel[[:blank:]]*for' {INPUT})| egrep 'schedule[[:blank:]]*\([[:blank:]]*dynamic' | wc -l" ,job,"Num dynamic schedule","scheduleDyn" ,True))
				statistics.append(Statistic("(egrep '#pragma[[:blank:]]*omp[[:blank:]]*for' {INPUT} && egrep '#pragma[[:blank:]]*omp[[:blank:]]*parallel[[:blank:]]*for' {INPUT})| egrep 'schedule[[:blank:]]*\([[:blank:]]*guided' | wc -l" ,job,"Num guided schedule","scheduleGui" ,True))
				statistics.append(Statistic("(egrep '#pragma[[:blank:]]*omp[[:blank:]]*for' {INPUT} && egrep '#pragma[[:blank:]]*omp[[:blank:]]*parallel[[:blank:]]*for' {INPUT})| egrep 'schedule[[:blank:]]*\([[:blank:]]*runtime' | wc -l" ,job,"Num runtime schedule","scheduleRun" ,True))

				#count of not recognized omp pragmas, MUST BE THE LAST IN LIST!!
				statistics.append(Statistic("echo ",job,"\\hspace{0.5cm}Num unknown omp","ompUnknown",True))	
				
			rows=""		#latexRows
			caption="{Statistics for "+os.path.abspath(header).replace("_","\_")+"}"
			
			countOmp=0		#values to count all omp pragmas and get not recognized ones		
			numOmp=0				
			for k in statistics:
				if(k.shortName=="ompUnknown"):
					k.command=k.command+str(numOmp-countOmp)
					
				latex=k.getLatex()
				result=k.execute()
				txtHeader=txtHeader+k.shortName+"\t"
				if(result.endswith("\n")):
					result=result[:-1]
				txtRow=txtRow+result[:7]+"\t"

				if latex==-1 or result==-1:
					print("ERROR executing statistic ",k.name)
				elif not latex=="":
					rows=rows+str(latex)

				if(k.shortName=="LOC"):
					suites[suite][job.data.path]["LOC"]=result
				if(k.shortName=="size"):
					suites[suite][job.data.path]["size"]=result					
				if(k.shortName=="numOmp"):
					suites[suite][job.data.path]["NumOMP"]=result
					numOmp=int(result)
				if(k.shortName.startswith("omp")):
					countOmp=countOmp+int(result)
					
				if(len(k.shortName)>7):				#for better readability introduce additional tabs
					txtRow=txtRow+"\t"				
				
			#header for txt/csv
			shortHeader=shortPath(header)
			header=os.path.abspath(header)
			label=header
			# Mark in doc that test failed
			if not success:
				shortHeader=shortPath(header+"-FAIL")
				header=header+" --- TEST FAILED"
				#overviewStatistics[job.data.path + " -- FAILED"] = overviewStatistics.pop(job.data.path)
			
			if(suites[suite][job.data.path]["Type"]=="ompOrSeq"):          #check if omp or seq
				if(int(suites[suite][job.data.path]["NumOMP"])>0):
					suites[suite][job.data.path]["Type"]="OMP"
				else:
					suites[suite][job.data.path]["Type"]="sequential"				
			
			if('sql' in stat_output):
				sqlStatement=""

				testHash=str(hashlib.md5(str(job.data.path).encode('utf-8')).hexdigest())
				
				failStateRef=""
				failStateVal=""
				
				if not (suites[suite][job.data.path]["failState"]==""):
					failStateRef=",failState"
					failStateVal="\",\""+suites[suite][job.data.path]["failState"]
				
				absPath=os.path.abspath(job.data.path)
				#get repo and path based on absolute path
				repo=""
				path=""
				
				#delete test data
				if overwriteSQL:
					deleteTest=sqlDeleteTest.format(**({
						'ID':testHash													
					}))
					sqlStatement+=deleteTest
				
				insertTest=sqlInsertTest.format(**({
					'ID':testHash,
					'NAME':shortPath(job.data.path),
					'PATH':path,
					'REPOSITORY':repo,
					'CODETYPE':suites[suite][job.data.path]["CodeStyle"],
					'LOC':suites[suite][job.data.path]["LOC"],
					'SIZE':suites[suite][job.data.path]["size"][:-1],
					'OMPPRAGMAS':suites[suite][job.data.path]["NumOMP"],
					'PARTYPE':suites[suite][job.data.path]["Type"],
					'FAILSTATE':suites[suite][job.data.path]["failState"]
				}))

				backendCompiler=job.backendCompiler
				
				backendHash=str(hashlib.md5((str(backendCompiler['Name'])+str(backendCompiler['Version'])).encode('utf-8')).hexdigest())
				insertBackend=sqlInsertBackendCompiler.format(**({
					'ID':backendHash,
					'NAME':backendCompiler['Name'],
					'VERSION':backendCompiler['Version']
				}))
				sqlStatement+=insertBackend+insertTest
				
				for key,val in totalTimes.items():
					
					#to prevent multiple insert metric statements save already inserted metrics 
					for k in threadRuns:
						if not "r"+str(k) in val:
							break;
						type=key
						numThreads=k
						runHash=str(hashlib.md5((testHash+str(type)+str(numThreads)).encode('utf-8')).hexdigest())
						
						insertRunConf=sqlInsertRunConfiguration.format(**({
							'ID':runHash,
							'TESTID':testHash,
							'TYPE':type,
							'NUMTHREADS':numThreads,
							'INSIEMEVERSION':INSIEME_VERSION,
							'BACKENDCOMPILER':backendHash,
							'HOST':host
						}))
						timeStamp=time.strftime("%Y-%m-%d %H:%M:%S")
						sqlStatement+=insertRunConf
						
						for metricKey,metricVal in val.items():
							if str(metricKey).endswith(str(k)):
								metricName=metricKey[:-1]							
																
								#remove short names 
								if(metricName=="mt"):
									metricName="MEMORY_TRANSFER"
								elif(metricName=="cr"):
									metricName="CACHE_RATE"
								elif(metricName=="r"):
									metricName="RUNTIME"
								elif(metricName=="m"):
									metricName="MEMORY_CONSUMPTION"
								#reference point, speedup, efficiency, cacheLineSize are not saved into db, flops are already here in perf metric								
								elif (metricName=="rp" or metricName=="s" or metricName=="e" or metricName=="LL_CACHE_LINE_SIZE" or metricName=="f"):	
									metricName=""
							else:
								#do not save statistics in db (medEff, maxEff, etc.)		
								metricName=""

							if(not metricName==""):
								if (not metricName in knownMetrics):
									knownMetrics.append(metricName)
									insertMetric=sqlInsertMetric.format(**({
										'NAME':metricName
									}))
									sqlStatement+=insertMetric
								
								
								#TODO COLLISIONS IN RESULTHASH!!!
								resultHash=str(hashlib.md5((metricName+runHash+timeStamp).encode('utf-8')).hexdigest())
								insertResult=sqlInsertResult.format(**({
									'ID':resultHash,
									'METRICID':metricName,
									'RUNID':runHash,
									'VALUE':metricVal,
									'EXECUTIONDATE':timeStamp
								}))
								sqlStatement+=insertResult
				
				print(sqlStatement)
				fileSql.write(sqlStatement)
				fileSql.flush()
				
				
			
			if('tex' in stat_output) or ('pdf' in stat_output):
				tableStatistics=tableEnv.format(**({'COLUMN_SPEC':'{|l|l|}','COLUMN_HEADERS':'\\textbf{Name} & \\textbf{Value}','ROWS':rows,'CAPTION':caption}))
				latexCode=latexCode+testEnv.format(**({'HEADER':"{"+header.replace("_","\_")+"}",'LABEL':"{"+label+"}",'RUNTIME_TABLE':tableRuntime,'STATISTICS':tableStatistics}))
			if('txt' in stat_output):
				if not (headerWritten):
					fileTxt.write(txtHeader+"\n")
				
				txtName=""
				
				leng=(len(suite)+len(shortHeader))
				if(leng>22):
					txtName=(suite+":"+shortHeader)[:21]
				elif(leng>14):
					txtName=suite+":"+shortHeader+"\t"
				elif(leng>6):
					txtName=suite+":"+shortHeader+"\t\t"
				elif(leng<=6):
					txtName=suite+":"+shortHeader+"\t\t\t"
				txtRow=txtRow.format_map( defaultdict(lambda: 'NA',{'name':txtName}))
				fileTxt.write(txtRow+"\n")
				fileTxt.flush()
			if('csv' in stat_output):
				if not (headerWritten):
					fileCsv.write(txtHeader.replace("\t\t\t",",").replace("\t",",")[:-1]+"\n")
				fileCsv.write(txtRow.replace("\t\t\t",",").replace("\t\t",",").replace("\t",",")[0:-1]+"\n")
				fileCsv.flush()
			
			if('bin' in stat_output):# and success:
				job.statistics=statistics
				pickle.dump(job,fileBin)
				fileBin.flush()
			headerWritten=True

	if not mock_run:
		# Before exiting the reporter prints a summary of the test case 
		col_print("\n#{0:~^{1}}#\n".format(' INTEGRATION TEST SUMMARY ', COLUMNS-2), BOLD)
		col_print("# SUCCESSFUL: ", BOLD)
		col_print("{0:>64} ".format(success_count), GREEN)
		col_print("#\n", BOLD)
		col_print("# FAILED:     ", BOLD)
		col_print("{0:>64} ".format(len(failedTests)), RED)
		col_print("#\n", BOLD)
		for test in failedTests:
			col_print("# -> ", BOLD)
			col_print("{0}\n".format(test), RED)
		col_print("#"+"~"*(COLUMNS-2)+"#\n", BOLD)
	

	# print the summary table 
	# print_summary(benchmarks_data)

	report_queue.put( len(failedTests) )
	
	#build latex document
	if(enable_stat and not mock_run and ('tex' in stat_output or 'pdf' in stat_output)):
		docEnv=("\\documentclass[12pt]{{article}}\n"
			"\\usepackage[margin=0.2in,landscape,a4paper]{{geometry}}\n"
			"\\usepackage{{hyperref}}\n"
			"\\usepackage{{float}}\n"
			"\\usepackage{{longtable}}\n"
			"\\begin{{document}}\n"
			"\\begin{{center}}\n"
			"{CONTENT}"
			"\\end{{center}}"
			"\\end{{document}}")
		
		#create summary table
		#rowsSummary=""
		rowsC=""
		rowsCPP=""		
		rowsUnknown=""
		rowsUnknownStatic=""
		rowsCStatic=""
		rowsCPPStatic=""
		rowsUnknownDynamic=""
		rowsCDynamic=""
		rowsCPPDynamic=""
		colheadersSummary="Name & seqRun & LOC & parType & effGCC & effInsieme & memGCC & memIns"
		
		colspecStatic="{|l|r|r|r|r|r|r|r|r|}"
		colheadersStatic="Name & LOC & size & parType & GFLOPS & memTrans & boundness & CMR & failsAt"
		
		colspecDynamic="{|l|r||r|r|r||r|r|r||r|r|r||r|r|r|}"
		colheadersDynamic="Name & seqRun & \\multicolumn{3}{|c||}{effGcc} & \\multicolumn{3}{|c||}{effIns} & \\multicolumn{3}{|c||}{memGcc} & \\multicolumn{3}{|c|}{memIns}"
				
		rowsStatic=""
		rowsDynamic="&&MIN&MED&MAX&MIN&MED&MAX&MIN&MED&MAX&MIN&MED&MAX"
		addRowsDyn=""	#help variable for the 1st row
		if(enable_sched):
			colheadersSummary=colheadersSummary+" & Dyn/Ins & Guid/Ins & Stat/Ins"
			
			colspecDynamic=colspecDynamic[:-1]+"r|r|r|}"
			colheadersDynamic=colheadersDynamic+" & Dyn/Ins & Guid/Ins & Stat/Ins"
			addRowsDyn="&&&"		#help variable for the 1st row
		colheadersDynamic=colheadersDynamic+"\\\\\\hline\n&&MIN&MED&MAX&MIN&MED&MAX&MIN&MED&MAX&MIN&MED&MAX"+addRowsDyn
		
		for suite,stat in suites.items():
			for key,value in stat.items():
				rowsDynamic=""
				rowsStatic="\n"
				rowsStatic=rowsStatic+suite+" : "+"\\hyperref["+os.path.abspath(key)+"]{"+shortPath(key).replace("_","\_")+"}"				
					
				if value["Type"]=="OMP" and not str(value["GCCMinEff"])=="":   #only include runs with multiple threads
					rowsDynamic="\n"+suite+" : "+"\\hyperref["+os.path.abspath(key)+"]{"+shortPath(key).replace("_","\_")+"}"
					rowsDynamic=rowsDynamic+" & "+str(value["seqRun"]).replace("\n","")			
					rowsDynamic=rowsDynamic+" & "+str(value["GCCMinEff"]).replace("\n","")
					rowsDynamic=rowsDynamic+" & "+str(value["GCCMedEff"]).replace("\n","")
					rowsDynamic=rowsDynamic+" & "+str(value["GCCMaxEff"]).replace("\n","")
					rowsDynamic=rowsDynamic+" & "+str(value["InsMinEff"]).replace("\n","")
					rowsDynamic=rowsDynamic+" & "+str(value["InsMedEff"]).replace("\n","")
					rowsDynamic=rowsDynamic+" & "+str(value["InsMaxEff"]).replace("\n","")
					rowsDynamic=rowsDynamic+" & "+str(value["GCCMinMem"]).replace("\n","")
					rowsDynamic=rowsDynamic+" & "+str(value["GCCMedMem"]).replace("\n","")
					rowsDynamic=rowsDynamic+" & "+str(value["GCCMaxMem"]).replace("\n","")
					rowsDynamic=rowsDynamic+" & "+str(value["InsMinMem"]).replace("\n","")
					rowsDynamic=rowsDynamic+" & "+str(value["InsMedMem"]).replace("\n","")
					rowsDynamic=rowsDynamic+" & "+str(value["InsMaxMem"]).replace("\n","")
					if(enable_sched):
						rowsDynamic=rowsDynamic+" & "+str(value["Dyn/Stat"]).replace("\n","")
						rowsDynamic=rowsDynamic+" & "+str(value["Guid/Stat"]).replace("\n","")
						rowsDynamic=rowsDynamic+" & "+str(value["Stat/Stat"]).replace("\n","")
					rowsDynamic=rowsDynamic+"\\\\\\hline"
	
				
				rowsStatic=rowsStatic+" & "+str(value["LOC"]).replace("\n","")
				rowsStatic=rowsStatic+" & "+str(value["size"]).replace("\n","")
				
				if(value["Type"]=="OMP"):
					rowsStatic=rowsStatic+" & "+str(value["NumOMP"])+" ompPragmas"
				else:
					rowsStatic=rowsStatic+" & "+value["Type"].replace("\n","")
					
				rowsStatic=rowsStatic+" & "+str(value["flops"])
				rowsStatic=rowsStatic+" & "+str(value["memTrans"])
				rowsStatic=rowsStatic+" & "+str(value["boundness"])
				rowsStatic=rowsStatic+" & "+str(value["cacheRate"])
				rowsStatic=rowsStatic+" & \\verb|"+value["failState"]+"|"				

				rowsStatic=rowsStatic+"\\\\\\hline"
				
				if(value["CodeStyle"]=="C"):
					rowsCStatic=rowsCStatic+rowsStatic		
					rowsCDynamic=rowsCDynamic+rowsDynamic	
				elif (value["CodeStyle"]=="C++"):
					rowsCPPStatic=rowsCPPStatic+rowsStatic
					rowsCPPDynamic=rowsCPPDynamic+rowsDynamic
				else:
					rowsUnknownStatic=rowsUnknownStatic+rowsStatic
					rowsUnknownDynamic=rowsUnknownDynamic+rowsDynamic
					
				rowsStatic=""

		tableStaticC=longTableEnv.format(**({'COLUMN_SPEC':colspecStatic,'COLUMN_HEADERS':colheadersStatic,'ROWS':rowsCStatic,'CAPTION':"{Static characteristics of C codes}"}))
		tableStaticCPP=longTableEnv.format(**({'COLUMN_SPEC':colspecStatic,'COLUMN_HEADERS':colheadersStatic,'ROWS':rowsCPPStatic,'CAPTION':"{Static characteristics of CPP codes}"}))
		tableStaticUnkn=longTableEnv.format(**({'COLUMN_SPEC':colspecStatic,'COLUMN_HEADERS':colheadersStatic,'ROWS':rowsUnknownStatic,'CAPTION':"{Static characteristics}"}))
		
		tableDynamicC=""
		tableDynamicCPP=""
		tableDynamicUnkn=""
		if(len(rowsCDynamic)>0):
			tableDynamicC="\\subsubsection{OpenMP characteristics}\n"+longTableEnv.format(**({'COLUMN_SPEC':colspecDynamic,'COLUMN_HEADERS':colheadersDynamic,'ROWS':rowsCDynamic,'CAPTION':"{OpenMP characteristics of C codes}"}))
		if(len(rowsCPPDynamic)>0):
			tableDynamicCPP="\\subsubsection{OpenMP characteristics}\n"+longTableEnv.format(**({'COLUMN_SPEC':colspecDynamic,'COLUMN_HEADERS':colheadersDynamic,'ROWS':rowsCPPDynamic,'CAPTION':"{OpenMP characteristics of CPP codes}"}))
		if(len(rowsUnknownDynamic)>0):
			tableDynamicUnkn="\\subsubsection{OpenMP characteristics}\n"+longTableEnv.format(**({'COLUMN_SPEC':colspecDynamic,'COLUMN_HEADERS':colheadersDynamic,'ROWS':rowsUnknownDynamic,'CAPTION':"{OpenMP characteristics}"}))

		cTable=""
		cppTable=""
		unknTable=""
		if not rowsCStatic=="":
			cTable="\\subsection{C Codes}\n\\subsubsection{Static characteristics}\n"+tableStaticC+tableDynamicC
		if not rowsCPPStatic=="":
			cppTable="\\subsection{CPP Codes}\n\\subsubsection{Static characteristics}\n"+tableStaticCPP+tableDynamicCPP
		if not rowsUnknownStatic=="":
			unknTable="\\subsection{Unclassifieable (C or C++)}\n\\subsubsection{Static characteristics}\n"+tableStaticUnkn+tableDynamicUnkn
		
		notations="\\section{Notations}"+notationTable+usedPrefixes
		latexCode="\\section{Overall statistics}\n"+cTable+cppTable+unknTable+notations+latexCode
		
		#put the whole stuff into latex document
		file = open("statistics.tex", "w")

		file.write(docEnv.format(**({'CONTENT':latexCode})))
		file.close()

		#build pdf
		if('pdf' in stat_output):
			command=subprocess.Popen(["pdflatex","-halt-on-error","-file-line-error","statistics.tex"],stdout=subprocess.PIPE,stderr=subprocess.PIPE)
			(output, error) =command.communicate()
				
			if(command.returncode):
				print("ERROR IN PDFTOLATEX (see also file statistics.tex):")
				print(error.decode('utf8'))
				print(output.decode('utf8'))
				print("\n LATEX CODE:\n"+docEnv.format(**({'CONTENT':latexCode})))
			else:
				if 'Rerun LaTeX.' in open('statistics.log').read():	#rerun latex because of width of longtable
					command=subprocess.Popen(["pdflatex","-halt-on-error","-file-line-error","statistics.tex"],stdout=subprocess.PIPE,stderr=subprocess.PIPE)
					(output, error) =command.communicate()    				
		
		if(enable_stat and 'sql' in stat_output and not mock_run):
			fileSql.close()
		if(enable_stat and 'csv' in stat_output and not mock_run):
			fileCsv.close()
		if(enable_stat and 'txt' in stat_output and not mock_run):
			fileTxt.close()
		if(enable_stat and 'bin' in stat_output and not mock_run):
			fileBin.close()
			if(os.path.exists("statistics_bak.bin")):
				os.remove("statistics_bak.bin")

def exec_shell_cmd(cmd, env=dict()):
	
	save_env = { }
	# set env
	for (key,val) in env.items():
		save_env[key] = os.getenv(key)
		os.putenv(key, val);
	
	command = os.path.expandvars(cmd)
	pid = subprocess.Popen( shlex.split(command), stdout=subprocess.PIPE, stderr=subprocess.STDOUT)
	output = pid.communicate()[0];

	# reset env
	for key,val in env.items():
		var = save_env[key]
		if not var:
			os.unsetenv(key)
		else:
			os.putenv(key,var);

	return pid.returncode, output.decode("utf-8")

# Generic executor of commands
def runner( job):
	if mock_run:
		col_print("@{0:~^80}\n".format('~'), BOLD)
		col_print("@Benchmark: {0}\n".format(job.data.path), BOLD)

	for cmd in job.cmds:
		if mock_run and cmd.type[0] < 5:
			print(cmd.desc)
			command = os.path.expandvars(cmd.cmd)
			col_print(command+"\n", GREEN)
			continue

		if cmd.type == Commands.SUCC:
			job.data.reports.append( CmdExecReport(Commands.SUCC, '', 'DONE', 0, "", 0, 0.0) )
			break

		perf_cmd=""
		
		#set up perf command in this order: flops, readMiss, readAcc, preMiss, preAcc, writeMiss, writeAcc
		if(cmd.type==Commands.RUN and enable_perf):
			perf_cmd="perf stat -x ,"
			for (key,value) in perf_metrics.items():
				if not (key=="REFPOINT" or key=="LL_CACHE_LINE_SIZE"):
					perf_cmd += " -e r"+value[2:]
	
		# Add time/and memory measurements
		real_cmd = ("/software-local/time/bin/time -f \"TIME(%e):MEM(%M)\" "+perf_cmd+"{0}").format(cmd.cmd)

		if cmd.out_file:
			file=open(cmd.out_file,"w")
		try:
			(retcode, output) = exec_shell_cmd(real_cmd, cmd.env)
		except Exception as e:
			retcode = 1
			print("{0}".format(e))
			break
		
		time_spec = output.split('\n')[-2]

		m = re.match(r"^TIME\((?P<time>\d+(.\d+)?)\):MEM\((?P<mem>\d+)\)$", time_spec)

		elapsed = -1
		tot_mem = -1

		if m is not None:
			elapsed = float(m.group('time'))
			tot_mem = int(m.group('mem'))

		perfEvents=dict();
		outputBack=output;
		if(cmd.type==Commands.RUN and enable_perf):
			lineNum=0
			for (key,value) in perf_metrics.items():
				if not (key=="REFPOINT" or key=="LL_CACHE_LINE_SIZE"):
					perfEvents[key]=output.split('\n')[-len(perf_metrics)+lineNum].split(',')[0]
					lineNum+=1
			output = "\n".join(output.split('\n')[:-len(perf_metrics)])
		else:
			output = "\n".join(output.split('\n')[:-2])			
			
		perfCode=0
		for (key,value) in perfEvents.items():
			if not (value.isdigit() or (value=="<not counted>")):
				retcode=1
				perfCode=1
				output=outputBack		#get whole output for user 
				cmd.cmd=perf_cmd+"\n"+cmd.cmd
				value=0
				break
		if(perfCode==1):
			perfEvents.clear()

		if cmd.out_file:
			file.write(output)
			file.close()

		# Create a report and append it to the result
		job.data.reports.append( CmdExecReport(cmd.type, cmd.cmd, cmd.desc, retcode, output, tot_mem, elapsed,cmd.threads,perfEvents=perfEvents) )

		if (cmd.type == Commands.COMPILE or cmd.type == Commands.RUN or cmd.type == Commands.HDIFF) and retcode != 0:
			job.data.reports.append( CmdExecReport(Commands.SUCC, '', 'DONE', 1, "", 0, 0.0,cmd.threads,cmd.desc,perfEvents) )
			# there was an error during compilation or running of the program, we mark the
			# test case as failed
			break
					
		# if the command is a RUN command and we have to perform multiple runs, then benchmarks this code
		if cmd.type == Commands.RUN and runs > 1:
			time = [ elapsed ] 
			mem = [ tot_mem ]
			for r in range(1,runs):
				(retcode, output) = exec_shell_cmd(real_cmd, cmd.env)
				if retcode != 0:
					# something went wrong, the program which was supposed to work is now failing
					job.data.reports[-1].ret_code = retcode
					job.data.reports[-1].output = output
					job.data.reports[-1].failState=cmd.desc
					break
				
				time_spec = output.split('\n')[-2]
		
				# print(time_spec)
				m = re.match(r"^TIME\((?P<time>\d+(.\d+)?)\):MEM\((?P<mem>\d+)\)$", time_spec)

				elapsed = float(m.group('time'))
				tot_mem = int(m.group('mem'))

				time.append( elapsed )
				mem.append( tot_mem )

			# compute mean value
			mean_time = sum(time)/runs			#changed to median but needed for stdev
			#mean_mem  = sum(mem)/runs
			
			median_time=median(time)
			median_mem=median(mem)
			
			job.data.reports[-1].time = median_time
			job.data.reports[-1].memory = median_mem
			# compute stdev
			job.data.reports[-1].stdev = math.sqrt(sum( map(lambda x: (x-mean_time)**2, time)) / runs)

		# time.sleep (5)
	if not mock_run:
		completed.put( job )

class Statistic:
	def __init__(self,command,job,name,shortName="",suppressZero=False):
		self.command=command
		self.suppressZero=suppressZero
		self.job=job
		self.name=name
		self._cachedValue=None
		if(shortName==""):
			self.shortName=name
		else:
			self.shortName=shortName
		
	#gets the total set of input files (inputs.data + files in current dir)
	def getInput(self):
		path=self.job.path
		inputs=self.job.files.split()
		
		files=[];
		#find all files in current dir with extension c, cpp, h, cc, hh
		for root,dirnames,filenames in os.walk(path):
			for filename in filenames:
				if(filename.endswith(('.c','.cpp','.h','.cc','.hh'))):
					files.append(os.path.abspath(os.path.join(root,filename)))
					
		#find all files in inputs.data and append them to the list
		for path in inputs:
			if not (path.startswith(("-I","-L","-D"))):
				absPath=os.path.abspath(path)
				if not (absPath in files):
					files.append(absPath)

		return files
		
	def execute(self):	
		if not(self._cachedValue==None):
			return self._cachedValue	
		if("INPUT" in self.command):
			sourceFiles=' '.join(self.getInput())
			command=subprocess.Popen(self.command.format(**({'INPUT':sourceFiles})),shell=True,stdout=subprocess.PIPE,stderr=subprocess.PIPE)
		else:
			command=subprocess.Popen(self.command,shell=True,stdout=subprocess.PIPE,stderr=subprocess.PIPE)
				
		(output, error) =command.communicate()

		if(command.returncode):
			print("ERROR in executing statistic "+self.name+":"+error.decode('utf8'))
			return -1
	
		self._cachedValue=output.decode('utf8')
		return output.decode('utf8')
	
	def getLatex(self):
		output=self.execute()
		if(self.suppressZero):
			if(output == str(0)+"\n"):
				return ""
		if(output==str(-1)+"\n"):
			return -1
		else:
			return self.name+" & "+str(output)+"\\\\\\hline\n"

class LOCStatistics(Statistic):
	def extractLOC(self,xmlString):
		xmldoc = ET.fromstring(xmlString)
		stat=xmldoc.find("languages")
		
		if not stat==None:
			tot=stat.find("total")
			if not tot==None:
				return tot.get("code")
			else:
				return -1
		else:
			return -1
		
	def execute(self):
		if not(self._cachedValue==None):
			return self._cachedValue	
		commandStr = '@CMAKE_SOURCE_DIR@'+"/test/cloc.pl {0} --quiet --xml"
		
		files=	' '.join(self.getInput())

		command=subprocess.Popen(commandStr.format(files),shell=True,stdout=subprocess.PIPE,stderr=subprocess.PIPE)
		(output, error) =command.communicate()
		loc="0"
		if not (output.decode('utf8')==""):		
			loc=self.extractLOC("\n".join((output.decode('utf8').split("\n")[1:])))		#remove blank line at the beginning

		if(command.returncode):
			print("ERROR in executing LOC statistic:"+error.decode('utf8'))
			return -1		
		
		if int(loc) >= 0:
			self._cachedValue=loc
			return loc
		elif loc == -1:
			print("ERROR in executing LOC statistic: XML received from CLOC not valid, xml file:"+output.decode('utf8')[1:])
			return -1	
		else:
			print("ERROR in executing LOC statistic:")
			return "-1"			

class Cmd:
	def __init__(self, type, desc, cmd, out_file=None, env=dict(),threads=-1,backendCompiler=dict()):
		self.type = type
		self.desc = desc
		self.cmd  = cmd
		self.out_file = out_file
		self.env = env
		self.threads=threads
		self.backendCompiler=backendCompiler


def load_from_file(file, path="./", default=''):
	if os.path.exists(file):
		return " ".join( [ x.strip() for x in open(file).read().format( 
			**{'SRC_DIR': SRC_DIR, 'PATH': path} ).split(',') ] )
	return default


def visit_files(trg_dir, passes=set()):
	if len(trg_dir) == 0:  return passes

	trg_dir = os.path.normpath(trg_dir)

	nexts = trg_dir.split('/')
	if nexts[0] == '':
		del nexts[0]
		nexts[0] = '/'+nexts[0]
	
	if len(nexts) == 0:  return passes

	final_specs = passes
	# check whethere there is a file pass in this subfolder
	if os.path.exists(nexts[0] + '/passes'):
		specs = [x.strip() for x in open(nexts[0]+'/passes').read().split()]
		for spec in specs:
			if spec == "all":
				final_specs |= set(passes_str)
				continue

			if spec.startswith('^'):
				final_specs -= { spec[1:] }
				continue

			final_specs |= {spec}

	os.chdir(nexts[0])
	return visit_files("/".join(nexts[1:]), final_specs)


def toAbsolutePaths(path, args):
	# Prepend to every file of this test case the absolute path which allows test case to be
	# executed from any directory of the system 
	abs_args = []
	for arg in args.split():
		arg = os.path.expandvars(arg)
		if arg.startswith('-I') and not os.path.isabs(arg[2:].strip()) and not arg[2:].strip().startswith('$'): 
			abs_args.append("-I{0}/{1}".format(path, arg[2:]))
			continue
		if arg.startswith('-L') and not os.path.isabs(arg[2:].strip()) and not arg[2:].strip().startswith('$'): 
			abs_args.append("-L{0}/{1}".format(path, arg[2:]))
			continue		
		if arg.startswith('--intercept'): 
			if arg.startswith('--intercept-include'): 
				abs_args.append("--intercept-include {0}".format(arg[19:]))
				continue
			if arg.startswith('--intercept'): 
				abs_args.append("--intercept {0}".format(arg[11:]))
				continue
		if not arg.startswith('-') and not os.path.isabs(arg):
			abs_args.append("{0}/{1}".format(path, arg))
			continue
		abs_args.append(arg)

	return " ".join(abs_args)

###################################################################################################
## TestJob
##
## Builds an object containing all the information related to a specific test. Contains its name
## and the list of commands which should be executed in order to perform the test.
###################################################################################################
class TestJob:

	def __init__(self, test_case_name):
		self.statistics=None
		
		save_path = os.path.abspath(os.getcwd())

		if test_case_name.endswith('/'): 
			test_case_name=test_case_name[:-1]
		
		if "$" in test_case_name:
			benchmarkSuite= test_case_name[test_case_name.find("$")+1:]
			test_case_name=	test_case_name[:test_case_name.find("$")]

		# save the absolute path to the test case before we switch directory 

		abs_path = os.path.abspath(test_case_name)
		# check which passes are enabled and set the directory to the test case 
		pas = visit_files(test_case_name)

		test_case_name = os.path.relpath(abs_path, save_path) 

		isCxx = False

		cur_passes = []
		for id, val in passes:
			if id in pas: 
				cur_passes.append( (id, val) )
				if id == "g++" or id == "icpc" or id.startswith("cxx"): isCxx = True

		self.__data = TestData(test_case_name,benchmarkSuite)
		self.__cmds = [ ]
		self.__is_parallel = False
		#ext = '.cpp' if isCxx==True else '.c'
		if isCxx == True:
			if os.path.exists(self.__data.name + '.cpp'):
				ext = '.cpp'
			else:
				ext = '.c'
		else:
			if os.path.exists(self.__data.name + '.c'):
				ext = '.c'
			else:
				ext = '.cpp'


		path = os.path.abspath(".")

		self.cur_passes=cur_passes	#added by kofler to support cleanup in destructor 
		self.path=path

		inputs 		 = load_from_file(INPUTS_DATA, path, self.__data.name + ext)
		insiemeFlags = toAbsolutePaths(path, load_from_file(INSIEME_FLAGS, path))
		gccFlagsRef  = toAbsolutePaths(path, load_from_file(REF_GCC_FLAGS, path))
		gccFlagsTest = toAbsolutePaths(path, load_from_file(TEST_GCC_FLAGS, path))

		input_args = None
		if os.path.exists(PROG_INPUTS):
			input_args = open(PROG_INPUTS).read().strip()

		abs_input_files = toAbsolutePaths(path, inputs)
		self.files=abs_input_files

		finalize_cmds = []
		
		first = True 
		ref = None

		schedules=[]
		if(enable_sched):
			schedules.append("IRT_STATIC")
			schedules.append("IRT_DYNAMIC")
			schedules.append("IRT_GUIDED")
	#	env_vars["IRT_LOOP_SCHED_POLICY"]="IRT_DYNAMIC"
					
		omp=False
		self.backendCompiler=0
		# Compile for selected backends (without rerunning semantic checks)
		for id, P in cur_passes:
			threadRunsLoc=threadRuns
			env_vars = dict()
			env_vars["GOMP_CPU_AFFINITY"]="0-1000"
			env_vars["OMP_NUM_THREADS"]="1"		#default nr of threads is 1
			env_vars["IRT_NUM_WORKERS"]="1"			
			
			if(enable_sched):
				if(id=="c_run"):							#clone insieme runs with different sched_policies
					cur_passes.append(("c_run_stat",P))
					cur_passes.append(("c_run_dyn",P))
					cur_passes.append(("c_run_guid",P))
				elif(id=="c_run_stat"):						#do static policy
					env_vars["IRT_SCHED_POLICY"]="IRT_SCHED_POLICY_STATIC"
					env_vars["IRT_LOOP_SCHED_POLICY"]="IRT_STATIC"
					threadRunsLoc=["stat;"+str(threadRuns[-1])]					
				elif(id=="c_run_dyn"):						#do dyn policy
					env_vars["IRT_SCHED_POLICY"]="IRT_SCHED_POLICY_STATIC"
					env_vars["IRT_LOOP_SCHED_POLICY"]="IRT_DYNAMIC"
					threadRunsLoc=["dyn;"+str(threadRuns[-1])]										
				elif(id=="c_run_guid"):						#do guid policy
					env_vars["IRT_SCHED_POLICY"]="IRT_SCHED_POLICY_STATIC"
					env_vars["IRT_LOOP_SCHED_POLICY"]="IRT_GUIDED"			
					threadRunsLoc=["guid;"+str(threadRuns[-1])]					
				
			if("fopenmp" in gccFlagsRef) and ("omp-sema" in insiemeFlags) and len(threadRuns)>1:
				omp=True			
			if (id=="c_seq"):
				omp=False
				
			if P.frontend:
				self.__cmds.append(
					Cmd(Commands.COMPILE, P.frontend.desc,
					"{PASS_FLAGS} {FLAGS} {INPUTS}".format(
						**{	'FLAGS': 		insiemeFlags,
							'INPUTS': 		abs_input_files,
							'PASS_FLAGS':	"{0}".format(P.frontend).format(**{
								'CASE_NAME': self.__data.name,
								'PATH': path
							})
						}),
					env=env_vars
					)
				)

				# Check diffs between generated files and reference files for this pass
				for file in P.frontend.files:
					file_name = ("{0}".format(file)).format(**{
						'CASE_NAME': self.__data.name,
						'PATH': path
					})

					ext = os.path.split(file_name)[1].split('.')[-1]
					abs_name = '/'.join( [os.path.split(file_name)[0], \
								'.'.join(os.path.split(file_name)[1].split('.')[0:-1])])

					ref_file = "{0}.ref.{1}".format(abs_name, ext)
					# if we have a reference file, then check with a DIFF
					if os.path.exists(ref_file):
						self.__cmds.append(
							Cmd(Commands.SDIFF, "Comparing with reference {0}".format(id),
							 "{0}/test/sortdiff {1}/{2} {1}/{3}".format(SRC_DIR, path, file_name,
								 ref_file),
			 					env=env_vars
							)
						)
						# If everything worked out, we overwrite the ref file 
						finalize_cmds.append( 
							Cmd(Commands.COPY, "Rewriting file as ref file {0}".format(id),
							 "mv {0}/{1} {0}/{2}".format(path, file_name, ref_file),
		 					env=env_vars
							)
						)

					else:
						self.__cmds.append(
							Cmd(Commands.COPY, "Copying file as ref file {0}".format(id),
							 "cp {0}/{1} {0}/{2}".format(path, file_name, ref_file),
		 					env=env_vars
							)
						)

			if P.backend:
				# Compile generated code
				input_file = "{0}/{1}".format(path, P.tmp_file.format(
								**{'CASE_NAME': self.__data.name, 'PATH': path}
							)) if P.tmp_file else abs_input_files

				file_name = ("{0}".format(P.backend.files[0])).format(**{
								'CASE_NAME': self.__data.name,
								'PATH': path
							})
				
				#set backendCompiler
				if self.backendCompiler==0:
					backComp=dict()
					backComp["Name"]= os.path.basename(str(P.backend).split(' ')[0])
					(retcode, output) = exec_shell_cmd(str(P.backend).split(' ')[0]+" -dumpversion")
					backComp["Version"]=output[:-1]					
					self.backendCompiler=backComp
				
				self.__cmds.append(
					Cmd(Commands.COMPILE, P.backend.desc,
					"{BACKEND} {FILE} {LDFLAGS} {GCCFLAGS}".format(
						**{	'BACKEND':	 "{0}".format(P.backend).format(**{
												'CASE_NAME': self.__data.name, 
												'PATH': 	path,
												'SRC_DIR': 	SRC_DIR
											}),
							'FILE': 	 input_file,
							'LDFLAGS':	 LDFLAGS_GCC,
							'GCCFLAGS':	 gccFlagsTest if P.frontend else gccFlagsRef
						}),
	 					env=env_vars
					)
				)

				# Run the generated executable
				executable = P.backend.output_file[0].format(**{
				  	 			'CASE_NAME': self.__data.name,
				  	 			'PATH': path
				  	 		})

				file_name ="{0}".format(executable)
				
				if input_args:
					exec_idx = input_args.index("{PATH}/{EXEC}")
					assert exec_idx != -1
					# use a regexp for recongnizing env variables 
					if exec_idx > 0 and '=' in input_args[0:exec_idx]:
						# it means we have some environment variable to set 
						env_vars = { x.split('=')[0].strip() : x.split('=')[1].strip() \
								 for x in input_args[0:exec_idx].split() if '=' in x }
						input_args = input_args[exec_idx:]

				if(omp):
					for thr in threadRunsLoc:	
						env_vars_copy=env_vars.copy()
						env_vars_copy["OMP_NUM_THREADS"]=str(thr).split(";")[-1]
						env_vars_copy["IRT_NUM_WORKERS"]=str(thr).split(";")[-1]
	
						self.__cmds.append(
							Cmd(Commands.RUN, "Run {0}".format(id),
								"{0} {1} {2}".format(
									P.exec,
									'-machinefile {0}/hosts'.format(path) if os.path.exists(path+'/hosts') and
									P.exec.strip() != ''  else '',
									input_args.format(**{'PATH':path, 
														 'EXEC': executable,
														 'THREADS': str(thr)
														})
									if input_args else "{0}/{1}".format(path, executable)
							 	  ),
								"{0}/{1}.thr{2}.out".format(path, file_name,thr), 
								env_vars_copy,
								thr
							)
						)
				else:
					self.__cmds.append(
							Cmd(Commands.RUN, "Run {0}".format(id),
								"{0} {1} {2}".format(
									P.exec,
									'-machinefile {0}/hosts'.format(path) if os.path.exists(path+'/hosts') and
									P.exec.strip() != ''  else '',
									input_args.format(**{'PATH':path, 
														 'EXEC': executable,
														 'THREADS': 1
														})
									if input_args else "{0}/{1}".format(path, executable)
							 	  ),
								"{0}/{1}.thr1.out".format(path, file_name), 
								env_vars
							)
						)		
					
			
				#if not mock_run:
				#	# upon finalization, remove the generated output files 
				#	finalize_cmds.append(
				#		Cmd(Commands.DEL, "Delete output file {0}".format(id),
				#			"rm {0}/{1}".format(path, file_name)
				#		)
				#	)

				if os.path.exists("{0}/output.match".format(path)):

					# If we are interested to a specific pattern instead of the entire file, we issue a
					# command to extract the pattern 
					self.__cmds.append(
						Cmd(Commands.EXT, "Extract pattern {0}".format(id),
							"{0} {1}/{2}".format(open("{0}/output.match".format(path)).read().strip(),
							path, file_name),
							"{0}/{1}.match".format(path, file_name),
							env=env_vars
						)
					)
					file_name ="{0}.match".format(file_name)
	
					#if not mock_run:
					#	# upon finalization, remove the generated output files 
					#	finalize_cmds.append(
					#		Cmd(Commands.DEL, "Delete output file {0}".format(id),
					#			"rm {0}/{1}".format(path, file_name)
					#		)
					#	)

				if first:
					ref = file_name

				# if we have a reference file, then check with a DIFF
				if(omp):
					for thr in threadRunsLoc:
						if not first:
							self.__cmds.append(
								Cmd(Commands.HDIFF, "Comparing output {0}".format(id),
								"{0}/test/sortdiff {1}/{2}.out {1}/{3}.out".format(SRC_DIR, path, ref+".thr1", file_name+".thr"+str(thr)),
								threads=thr,
								env=env_vars
							))
						first = False
				else:
						self.__cmds.append(
									Cmd(Commands.HDIFF, "Comparing output {0}".format(id),
									"{0}/test/sortdiff {1}/{2}.out {1}/{3}.out".format(SRC_DIR, path, ref+".thr1", file_name+".thr1"),
									env=env_vars
									)
						)
						first = False

		self.__cmds += finalize_cmds
		self.__cmds.append(
			Cmd(Commands.SUCC, "Set as Successful","")
		)
		os.chdir(save_path)
		self.ref=ref
		self.omp=(("fopenmp" in gccFlagsRef) and ("omp-sema" in insiemeFlags))

	def __str__(self):
		return "\n\n".join( map(lambda x: "# {0} -> {1}\n{2}".format(x.type, x.desc, x.cmd), self.__cmds) )

	@property
	def cmds(self): return self.__cmds # sorted(self.__cmds, key=lambda x: x.type)

	@property
	def data(self): return self.__data

	@property
	def inputs(self): return self.files

	def __del__(self):		#introduced by kofler to support cleanup, not very elegant but im no python pro ;)
		if do_clean is True:
			#print("Removing all generated files")
			for id, P in self.cur_passes:
				if P.frontend is not None:
					for file in P.frontend.files:
						ext = os.path.split(file)[1].split('.')[-1]
						abs_name = '/'.join( [os.path.split(file)[0], \
							'.'.join(os.path.split(file)[1].split('.')[0:-1])])
						nat_file = "{0}.{1}".format(abs_name, ext).format(**{
							'CASE_NAME': self.__data.name,
							'PATH': self.path
						})

						ref_file = "{0}.ref.{1}".format(abs_name, ext).format(**{
							'CASE_NAME': self.__data.name,
							'PATH': self.path
						})
						ref_file=self.path+ref_file
						nat_file=self.path+nat_file
						match_file=self.path+ref_file+".match"						
						if os.path.exists(ref_file): os.remove(ref_file)
						if os.path.exists(nat_file): os.remove(nat_file)
						if os.path.exists(match_file): os.remove(match_file)
							

				if P.backend is not None:
					for file in P.backend.files:
						ext = os.path.split(file)[1].split('.')[-1]
						abs_name = '/'.join( [os.path.split(file)[0], \
							'.'.join(os.path.split(file)[1].split('.')[0:-1])])
						nat_file="{0}.{1}".format(abs_name,ext).format(**{
							'CASE_NAME': self.__data.name,
							'PATH': self.path
						})
						
						for thr in threadRuns:
							ref_file = "{0}.{1}".format(abs_name,ext).format(**{
								'CASE_NAME': self.__data.name,
								'PATH': self.path,
							})
							ref_file_thr=self.path+ref_file+".thr"+str(thr)+".out"
							if os.path.exists(ref_file_thr): os.remove(ref_file_thr)
						nat_file=self.path+nat_file
						match_file=self.path+ref_file+".match"
						if os.path.exists(nat_file): os.remove(nat_file)
						if os.path.exists(match_file): os.remove(match_file)

def main(argv=None):

	# setput enviroment variables for spetial libraries lookup
	os.environ['BOOST_ROOT'] = '@BOOST_ROOT@'
	os.environ['LD_LIBRARY_PATH'] = '@BOOST_ROOT@/lib:'+os.environ['LD_LIBRARY_PATH']

	config = configparser.ConfigParser()
	config.readfp(open('@CMAKE_BINARY_DIR@/test.cfg'))
	
	supportedOutputFormats=('pdf','tex','txt','csv','bin','sql')

	# Global variable which stores the list of backends to be used
	global passes
	global threadRuns
	global passes_str

	# Load passes from the configuration file 
	passes = [(key[8:],eval(val)) for key,val in config.items("Test") if key.startswith('pass')]
	passes_str = [id for id,p in passes]

	parser = optparse.OptionParser()
	parser.add_option("-b", "--backends", dest="backends", type="string",
							help=("The list of backends/passes that should be used: "
								"options are {0},'all', default: '%default'.".
								format(", ".join(list(map(lambda x: "\'"+x+"\'", passes_str))))),
							default="all")

	parser.add_option("-w", "--workers", dest="workers", type="int",
							help=("The number parallel workers: default: '%default'."),
							# default=os.sysconf("SC_NPROCESSORS_ONLN")
							default=1)

	parser.add_option("-c", "--clean", action="store_true", dest="clean",
							help=("Cleanup reference files, default= %default"),
							default=False)

	parser.add_option("-m", "--mock-run", action="store_true", dest="mock_run",
							help=("Execute the mock run (you know what I mean don't let "
							"me waste time explaining it :), default= %default"),
							default=False)

	parser.add_option("-s","--statistics",action="store_true", dest="stat_enabled",
							help=("Execute the statistics mode, saves code statistics into specified output formats."
								" Uses by default perf to measure cpu counters, define experiments in perf_counters.dat! Default= %default"),
							default=False)

	parser.add_option("-r", "--runs", dest="runs", type="int",
							help=("Execute the binary multiple times and produce a report on"
							" median execution time and standard deviation , default= %default"),
							default=1)
	
	parser.add_option("", "--no-perf", action="store_true", dest="perf_disabled",
							help=("Disable perf statistics even if statistics are enabled"),
							default=False)
	
	parser.add_option("-S","--scheduling",action="store_true", dest="sched_enabled",
							help=("Execute using different scheduling variants for insieme, default= %default"),
							default=False)	
	
	parser.add_option("-o","--output-format",dest="outputFiles",type="string",
							help=("Set the output formats of the statistics module seperated by ',', supported formats: "+",".join(supportedOutputFormats)+". default=%default"),
							default=("tex,pdf"))
	
	parser.add_option("","--no-overwrite",action="store_false",dest="overWriteSQL",
							help=("If generating sql output overwrite old results. default=%default"),
							default=True)
	
	parser.add_option("-t","--max-threads",dest="max_threads", type="int",
							help=("Execute each test case multiple times with a different number of threads (increased by ^2) until max threads is reached. Saves time measurements in specified output formats if statistics are enabled. default= %default"),
							default=0)

	parser.add_option("-f","--force",action="store_true",dest="force", 
							help=("Try to execute all tests in test.cfg (even commented with #). default= %default"),
							default=False)

	parser.add_option("-i","--input-file",dest="inputFile",type="string",
							help=("Use input file of previous runs (saved by using -o bin). Reads out job and statistics of the input file."),
							default="")

	parser.add_option("-v", "--verbose", dest="verbose", action="store_true",
							help=("Print to standard output occurred errors verbosily"
							", default=%default"),
							default=False)

	parser.add_option("--no-colors", dest="no_colors", action="store_true",
							help=("Disable color output, default=%default"),
							default=False)

	parser.add_option("--show-cases", dest="show_cases", action="store_true",
							help=("Display the list of test cases which will be executed, default=%default"),
							default=False)

	parser.add_option("--cfg", dest="cfg", type="string",
							help=("Loads the list of benchmarks to be executed through a file, default=%default"),
							default=None)

	(options, args) = parser.parse_args()

	save_path = os.getcwd()
	# Retrieve the insieme version 
	os.chdir('@CMAKE_SOURCE_DIR@')
	global INSIEME_VERSION
	(_, INSIEME_VERSION) = exec_shell_cmd('git describe --dirty')
	INSIEME_VERSION = INSIEME_VERSION.strip()
	
	msg_str = "Insieme version: {0}".format( INSIEME_VERSION )
	print("-"*COLUMNS)
	print("|{0:^{1}}|".format(msg_str, COLUMNS-2))
	print("|"+"-"*(COLUMNS-2)+"|")
	# restore path 
	os.chdir(save_path)

	# Set the number of workers
	num_cores = options.workers

	if options.cfg:
		args = read_test_cfg('./', options.cfg)

	# If the script is executed without specifying a path, the test folder
	# in the SOURCE directory is used as entry point 
	if not args: args = '@CMAKE_SOURCE_DIR@/test' 
	
	global force_tests
	force_tests=options.force

	assert(args)
	test_cases_orig = expand( args )
	
	test_cases=[]
	#create a second structure without testSuites
	for k in test_cases_orig:
		if "$" in k:
			test_cases.append(k[:k.find("$")])
		else:
			test_cases.append(k)
	assert(len(test_cases)==len(test_cases_orig))
	if options.show_cases:
		print (" ".join(test_cases))
	
	global test_cases_number
	test_cases_number = len(test_cases)
	
	global enable_stat
	enable_stat=options.stat_enabled	
	
	global enable_perf
	enable_perf=False
	if(enable_stat and not options.perf_disabled):
		enable_perf=True

	if enable_perf:
		global perf_metrics
		perf_metrics=collections.OrderedDict()	
		#read file
		filePerf = open('perf_counters.dat','r')
		lines = filePerf.readlines()
		filePerf.close()
		
		for line in lines:
			if not line.startswith("#") and not line=="\n":
				splits=line.split("\t")
				name=splits[0]
				val=splits[len(splits)-1][:-1]
				perf_metrics[name]=val
		
		#check if all required values here
		allVals= "FLOPS" in perf_metrics and "LL_CACHE_READ_MISS" in perf_metrics and "LL_PREFETCH_MISS" in perf_metrics and \
			"LL_CACHE_READ_ACCESS" in perf_metrics and "LL_PREFETCH_ACCESS" in perf_metrics and "LL_CACHE_WRITE_MISS" in perf_metrics and \
			"LL_CACHE_WRITE_ACCESS" in perf_metrics and "LL_CACHE_LINE_SIZE" in perf_metrics and "REFPOINT" in perf_metrics
		
		if not allVals:
			print ("Not all required perf metrics defined! Check perf_counters.dat!")
			exit()
		
	global enable_sched
	enable_sched=options.sched_enabled	

	print("|{0:^{1}}|".format("Running '{0}' benchmark(s)".format( len(test_cases) ), COLUMNS-2))
	print("|"+"-"*(COLUMNS-2)+"|")

	if test_cases_number == 0:
		print("|"+"-"*(COLUMNS-2)+"|")
		print("|{0:^{1}}|".format("DONE", COLUMNS-2))
		print("|"+"-"*(COLUMNS-2)+"|")
		return os._exit(0)

	threadRuns = []
	threadRuns.append(1)
	if not (options.max_threads == 0):
		threadNum = 2
		while threadNum <= options.max_threads:
			threadRuns.append(threadNum)
			threadNum = threadNum * 2
		if not (options.max_threads in threadRuns):
			threadRuns.append(options.max_threads)
		print ("Each OpenMP test (tests using -fopenmp AND --omp-sema) is executed with: {0} threads".format(threadRuns))

	if not (options.inputFile==""):		#use input file
		jobs=[]
		file=open(options.inputFile,"rb")
		try:
			while 1==1:
				jobs.append(pickle.load(file))
		except EOFError:
			file.close()

		test_cases_abs=[]
		for x in range(0,len(test_cases)):		#convert path to abs path to compare them with tests in file
			test_cases_abs.append(os.path.abspath(test_cases[x]))

		#look if data in input file is valid and sufficient
		for job in jobs:
			dataSufficient=True
			if(job.omp):
				for threadRun in threadRuns:
					if not threadRun in (node.threads for node in job.data.reports):
						dataSufficient=False
						#break
				if enable_sched:
					if not "dyn;"+str(threadRuns[-1]) in  (node.threads for node in job.data.reports):
						dataSufficient=False
					if not "stat;"+str(threadRuns[-1]) in  (node.threads for node in job.data.reports):
						dataSufficient=False
					if not "guid;"+str(threadRuns[-1]) in  (node.threads for node in job.data.reports):
						dataSufficient=False
			
			if(not dataSufficient and os.path.abspath(job.data.path) in test_cases_abs):	
				print("WARNING: Cached job "+job.data.name+" contains insufficient data, executing again!")
				if(os.path.abspath(job.data.path) in test_cases_abs):
					ind = test_cases_abs.index(os.path.abspath(job.data.path))
					del test_cases[ind]
					del test_cases_abs[ind]
					del test_cases_orig[ind]
					completed.put(job)				
			else:		
				if(os.path.abspath(job.data.path) in test_cases_abs):
					ind = test_cases_abs.index(os.path.abspath(job.data.path))
					del test_cases[ind]
					del test_cases_abs[ind]
					del test_cases_orig[ind]
					completed.put(job)

	# Read the backends provided by the user and select the one to be used in the run 
	backends = list({x.strip() for x in options.backends.split(',')})

	if not (len(backends) == 1 and backends[0] == "all"): 
		allowed = []
		# Filter backend
		for backend in backends:
			assert backend in passes_str
			allowed.append( passes_str.index(backend) )

		allowed = sorted(allowed)

		passes = [ passes[x] for x in allowed ]
		passes_str = [ x[0] for x in passes]
	
	# check if the intel compiler is installed, otherwise remove the icc from the backends 
	if not os.system("which gcc"):
		to_remove = []
		for x in range(len(passes)): 
			if passes_str[x].endswith("icc") or passes_str[x].endswith("icpc"):
				to_remove.append(x-len(to_remove))

		for x in to_remove:
			del passes[x]
			del passes_str[x]

	global no_colors
	no_colors=options.no_colors
				
	print(" * Selected passes are: {0}".
			format( ", ".join(list(map(lambda x: "'"+x+"'", passes_str) ))) 
		 )
	
	if((options.stat_enabled or options.max_threads>1) and options.workers>1):
		print("ERROR, multiple worker not allowed in statistics/multiple thread mode")
		#exit()
	if(options.stat_enabled and options.mock_run):
		print("WARNING: statistics are not written since running only a mock run")
	
	
	# Global variable which stores whether the reference files should be removed
	global do_clean
	do_clean=options.clean

	global mock_run
	mock_run=options.mock_run

	global runs
	runs=options.runs

	global verbose
	verbose=options.verbose
	
	global stat_output
	stat_output=options.outputFiles.split(',')

	global overwriteSQL
	overwriteSQL=options.overWriteSQL
	
	#print warning if outputFormat not supported
	for out in stat_output:
		if not (out in supportedOutputFormats):
			print("WARNING output format '"+out+"' not supported, format is ignored!\n")

	global SRC_DIR
	SRC_DIR=config.get("Test", "SRC_DIR")

	global BIN_DIR
	BIN_DIR=config.get("Test", "BIN_DIR")

	report_queue = Queue()

	# Instantiate the reporter
	t = Process(target=reporter, args=[report_queue] )
	t.start()

	if len(test_cases)>0:
		test_jobs = [TestJob(x) for x in test_cases_orig]

		# ldPath = os.getenv("LD_LIBRARY_PATH", "");
		os.putenv("SLOTS", "1");

		# instantiate the pool of threads which are used to run test cases
		pool = Pool(processes=num_cores)
		pool.map(runner, test_jobs,1)
		pool.close()
		pool.join()

	# make the reporter exit
	completed.put( None )
	t.join()
	# os.putenv("LD_LIBRARY_PATH",ldPath)

	t.join()

	ret_val = report_queue.get()
	return os._exit(ret_val)

if __name__ == "__main__":
	sys.exit(main())
